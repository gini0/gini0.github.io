{"pages":[],"posts":[{"title":"WP的buffer pool报错导致数据库Error的解决方法","text":"首先查看错误日志 12Cd /var/logTail -n 500 mysqld.log 发现错误是： 123456782018-11-11T09:08:20.364026Z 0 [Note] InnoDB: Initializing buffer pool, total size = 50M, instances = 1, chunk size = 50M2018-11-11T09:08:21.751200Z 0 [ERROR] InnoDB: mmap(53690368 bytes) failed; errno 122018-11-11T09:08:21.751262Z 0 [ERROR] InnoDB: Cannot allocate memory for the buffer pool2018-11-11T09:08:21.751274Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error2018-11-11T09:08:22.220815Z 0 [ERROR] Plugin &apos;InnoDB&apos; init function returned error.2018-11-11T09:08:22.526092Z 0 [ERROR] Plugin &apos;InnoDB&apos; registration as a STORAGE ENGINE failed.2018-11-11T09:08:22.526142Z 0 [ERROR] Failed to initialize builtin plugins.2018-11-11T09:08:22.526159Z 0 [ERROR] Aborting 导致的 Wp Error establishing a database connection problem解决方法：改mysql的设置 1234Where is my.cnfCd /etcVim my.cnfinnodb_buffer_pool_size = 50M 依然不行改swap空间空间太小，设置swap分区 参考网址：https://blog.csdn.net/Mr_OOO/article/details/78653523http://junhey.com/2016/06/16/2016-06-17-05-10.htmlhttps://www.cnblogs.com/LUO77/p/5821530.html 参考文件,这儿后面的就是在my.cnf可以直接修改23","link":"/2018/11/14/Wp中数据库Error的解决方法/"},{"title":"关于打开网页跳流氓广告解决方法","text":"​ 今天GM说我们想做的一个站点打开慢的一逼，然后我打开发现，跳出了4-5个流氓广告，惊了,这不是速度的事情。 大概思路是这么几个： ​ 1、网站被攻击，js注入 ​ 2、浏览器缓存有木马 ​ 3、路由被dns挟持或有后门 方法： 打开很多网站都发现有广告，而且是一样的。排除1 然后清理缓存，重置了浏览器问题依旧。基本判断是问题3 登录路由器，进入dns 江苏联通运营商的dns： 58.240.57.33 221.6.4.66 重新修改密码，重启。 问题解决！","link":"/2018/11/21/关于打开网页跳流氓广告解决方法/"},{"title":"“给我做一个类似XX的App”","text":"​ 刚刚看论坛，有人说有客户说到要做一个类似XX的App，当然遭遇的群嘲。无非就是没有具体需求，没有预算种种。Ref：https://www.v2ex.com/t/509918#reply40 ​ 我确实经常遇到这样的客户，我一般会听一听，然后觉得有可行性就做，或者我觉得项目挺好的也做，但是大部分我都不想做或者拒绝了，其实对于小的技术团队和个人来说是浪费精力和竞争力。简单来说就是10年经验，3年技术的现象，你会做的调用的就是那些东西，没啥意思。另一个方面，你一听，坦白说你就大概率知道明年就不能续费了，虽然可能会少很多钱，但是这一锤子买卖确实没的意思。 ​ 基本上创业的人都有一种或多或少改变世界的心，再退一点，就是我的产品不能改变世界，客户的产品牛逼了我也挺开心的。我听说现在做个公司官网就1200块钱还包服务器，有什么做头。 ​ 碎碎念一下，这个野蛮竞争确实没有意思，明年上Ai的生产环境又不知道什么可以落地，CV、NLP还是什么，但这最起码会让觉得有点做头和盼头吧，不然真是苟且生活了。 ​ 在我看来开源，改进轮子都是进步，这个壁垒其实比造轮子高；“做一个类似XX的App”，如果真的完全没有想法、资源，就是给钱无他，就是把客户割韭菜了。 ​ 最后就应该是一种合作共赢的关系，你找我，你的产品可以创造更多，获得更多，其实也就是其实找到好的技术公司应该是客户的幸运，当然我不是说我；我也可以积累行业经验、技术经验，一般这种，我都愿意很少收费或者某种入股来做，不然你直接淘宝做就是了，你那些东西，功能和需求，坦白说，我也做不出花来。 ​ 我自己公司官网写了几句话，“几包泡面，几行代码，一个想法“。就是这样，TF几十行代码就可以做出一个LSNTRNN还带BN云云其实都可以上生产了，你搞个几千行代码吓唬谁和装什么逼呢？今天看抖音有个人新建个文件夹还要终端mkdir，我看你装逼装的真是在家放个屁都要说一声Excuse me。 ​ 忠于生活，忠于理想，改变一点。碎碎念完了，继续看ML，继续想可以怎么实现落地怎么改变。 ​","link":"/2018/11/21/“给我做一个类似XX的App”/"},{"title":"怎么优化LAMP的速度","text":"1、Chrome 访问你的博客，接着 Chrome 的菜单 -》视图 -》开发者 -》开发者工具 -》 Network 刷新一次页面，根据页面加载各种内容的耗时对症下药。 2、服务器延迟，换服务器 3、是否页面 TTFB 过高 动静分离、静态文件未对国内优化、Gravatar 换成国内镜像，CDNJS 换成国内镜像， 静态资源与主站分离 4、php 用 memcached 和 opcache 网络配置换 google 的 bbr 5、 php 执行很慢，你得确认你 php 版本？尽可能上 php7，另外 php 的 APC 缓存加速开了么？ php7 和 apc 缓存 是否开，都直接影响页面执行性能若干倍。 页面和 php 能作的都作了，还是慢，那就得查 db 了。 MySQL 慢查询日志记录开了没？慢查询多不多？ mysql 版本？数据表存储引擎类型？ InnoDB 么？ Innodb mem pool size 够不够大？ phpmyadmin 查查 db 的最近执行性能统计信息。 进系统再查 内存是不是不够，swap 是否频繁读写？ vultr 后台再查查 测试期间 /慢的时候，系统性能 /网络是不是被吃满了？排除资源不够的问题。","link":"/2018/11/19/怎么优化LAMP的速度/"},{"title":"python 爬虫基础","text":"做了一个整理，把python的爬虫基础发一下。这个是基于周莫凡的python整理材料 [TOC] 网页爬虫了解网页结构用python登录网页：123456from urllib.request import urlopen# if has Chinese, apply decode()html = urlopen( \"https://morvanzhou.github.io/static/scraping/basic-structure.html\").read().decode('utf-8')print(html) 然后用正则表达式：12res = re.findall(r\"&lt;p&gt;(.*?)&lt;/p&gt;\", html, flags=re.DOTALL) # re.DOTALL if multi lineprint(\"\\nPage paragraph is: \", res[0]) BeautifulSoup 解析网页BS基础选着要爬的网址 (url)使用 python 登录上这个网址 (urlopen等)读取网页信息 (read() 出来)将读取的信息放入 BeautifulSoup使用 BeautifulSoup 选取 tag 信息等 (代替正则表达式)安装12# Python 3+pip3 install beautifulsoup4 12345678910111213141516from bs4 import BeautifulSoupfrom urllib.request import urlopen# if has Chinese, apply decode()html = urlopen(\"https://morvanzhou.github.io/static/scraping/basic-structure.html\").read().decode('utf-8')print(html)#这边的解析方式有很多 https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ 推荐使用lxmlsoup = BeautifulSoup(html, features='lxml')print(soup.h1) #直接h1print('\\n', soup.p)all_href = soup.find_all('a') #all_href = [l['href'] for l in all_href]print('\\n', all_href) BS 解析CSS12345678910111213141516171819#还是导入两个模块from bs4 import BeautifulSoupfrom urllib.request import urlopen# if has Chinese, apply decode()html = urlopen(\"https://morvanzhou.github.io/static/scraping/list.html\").read().decode('utf-8')print(html)soup = BeautifulSoup(html, features='lxml')# use class to narrow searchmonth = soup.find_all('li', {\"class\": \"month\"})for m in month: print(m.get_text()) #这边用get_text()来获得里面的具体信息jan = soup.find('ul', {\"class\": 'jan'})d_jan = jan.find_all('li') # use jan as a parentfor d in d_jan: print(d.get_text()) BS与正则表达式的运用 导入正则模块 re.compile() 迭代输出,输出的是列表1234567891011121314151617from bs4 import BeautifulSoupfrom urllib.request import urlopenimport re# if has Chinese, apply decode()html = urlopen(\"https://morvanzhou.github.io/static/scraping/table.html\").read().decode('utf-8')print(html)soup = BeautifulSoup(html, features='lxml')img_links = soup.find_all(\"img\", {\"src\": re.compile('.*?\\.jpg')})for link in img_links: print(link['src']) course_links = soup.find_all('a', {'href': re.compile('https://morvan.*')})for link in course_links: print(link['href']) 爬百度百科 重要https://morvanzhou.github.io/tutorials/data-manipulation/scraping/2-04-practice-baidu-baike/12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#Practice: scrape Baidu Baike#Here we build a scraper to crawl Baidu Baike from this page onwards. We store a historical webpage that we have already visited to keep tracking it.#In [2]:from bs4 import BeautifulSoupfrom urllib.request import urlopenimport reimport randombase_url = \"https://baike.baidu.com\"his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]#Select the last sub url in \"his\", print the title and url.url = base_url + his[-1]html = urlopen(url).read().decode('utf-8')soup = BeautifulSoup(html, features='lxml')print(soup.find('h1').get_text(), ' url: ', his[-1])#网络爬虫 url: /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711#Find all sub_urls for baidu baike (item page), randomly select a sub_urls and store it in \"his\". If no valid sub link is found, than pop last url in \"his\".# find valid urlssub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})if len(sub_urls) != 0: his.append(random.sample(sub_urls, 1)[0]['href'])else: # no valid sub link found his.pop()print(his)#['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E4%B8%8B%E8%BD%BD%E8%80%85']#Put everthing together. Random running for 20 iterations. See what we end up with.#his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]for i in range(20): url = base_url + his[-1] html = urlopen(url).read().decode('utf-8') soup = BeautifulSoup(html, features='lxml') print(i, soup.find('h1').get_text(), ' url: ', his[-1]) # find valid urls sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")}) if len(sub_urls) != 0: his.append(random.sample(sub_urls, 1)[0]['href']) else: # no valid sub link found his.pop() 更多请求/下载方式Request的方式Requests：get/post我们就来说两个重要的, get, post, 95% 的时间, 你都是在使用这两个来请求一个网页.post： 账号登录 搜索内容 上传图片 上传文件 往服务器传数据 等get： 正常打开网页 不往服务器传数据 网页使用 get 就可以了, 都是只是 get 发送请求. 而 post, 我们则是给服务器发送个性化请求, 比如将你的账号密码传给服务器, 让它给你返回一个含有你个人信息的 HTML. 从主动和被动的角度来说, post 中文是发送, 比较主动, 你控制了服务器返回的内容. 而 get 中文是取得, 是被动的, 你没有发送给服务器个性化的信息, 它不会根据你个性化的信息返回不一样的 HTML 安装Requests12# python 3+pip3 install requests get有一个参数 params，就是Request.post用Session来传递cookie,就是Session.get注意post的地址，get地址里面没有现实的参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#requests: an alternative to urllib#requests has more functions to replace urlopen. Use request.get() to replace urlopen() and pass some parameters to the webpage. The webbrowser is to open the new url and give you an visualization of this result.import requestsimport webbrowser #内置模块，打开浏览器param = {\"wd\": \"莫烦Python\"}r = requests.get('http://www.baidu.com/s', params=param)print(r.url)webbrowser.open(r.url)http://www.baidu.com/s?wd=%E8%8E%AB%E7%83%A6Python#Out[2]:#True##post##We test the post function in this page. To pass some data to the server to analyse and send some response to you accordingly.data = {'firstname': '莫烦', 'lastname': '周'}r = requests.post('http://pythonscraping.com/files/processing.php', data=data)print(r.text)#Hello there, 莫烦 周!##upload image##We still use post function to update image in this page.file = {'uploadFile': open('./image.png', 'rb')}r = requests.post('http://pythonscraping.com/files/processing2.php', files=file)print(r.text)#The file image.png has been uploaded.#login##Use post method to login to a website.payload = {'username': 'Morvan', 'password': 'password'}r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)print(r.cookies.get_dict())r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)print(r.text)#{'username': 'Morvan', 'loggedin': '1'}#Hey Morvan! Looks like you're still logged into the site!##another general way to login##Use session instead requests. Keep you in a session and keep track the cookies.session = requests.Session()payload = {'username': 'Morvan', 'password': 'password'}r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)print(r.cookies.get_dict())r = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")print(r.text)#{'username': 'Morvan', 'loggedin': '1'}#Hey Morvan! Looks like you're still logged into the site! 下载文件3种方式1234567891011121314151617181920#了下载到一个特定的文件夹, 我们先建立一个文件夹吧. 并且规定这个图片下载地址import osos.makedirs('./img/', exist_ok=True)IMAGE_URL = \"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png\"#方法1 使用 urlretrieve from urllib.request import urlretrieveurlretrieve(IMAGE_URL, './img/image1.png')#方法2 使用 request import requestsr = requests.get(IMAGE_URL)with open('./img/image2.png', 'wb') as f: f.write(r.content) #方法3 使用流的方式，不然只能先存在内存里r = requests.get(IMAGE_URL, stream=True) # stream loadingwith open('./img/image3.png', 'wb') as f: for chunk in r.iter_content(chunk_size=32): f.write(chunk) 循环下载图片requests 访问和 下载功能, 还有 BeautifulSoup、找到img_list下面。。。的img的src 找到图片位置，分析12345678910111213141516171819202122232425from bs4 import BeautifulSoupimport requestsURL = \"http://www.nationalgeographic.com.cn/animals/\"# find list of image holderhtml = requests.get(URL).textsoup = BeautifulSoup(html, 'lxml')img_ul = soup.find_all('ul', {\"class\": \"img_list\"})#Create a folder for these picturesimport osos.makedirs('./img/', exist_ok=True)#download#Find all picture urls and download them.In [4]:for ul in img_ul: imgs = ul.find_all('img')# find_all()函数 for img in imgs: url = img['src'] r = requests.get(url, stream=True) #用流的方式 image_name = url.split('/')[-1] with open('./img/%s' % image_name, 'wb') as f: for chunk in r.iter_content(chunk_size=128): f.write(chunk) print('Saved %s' % image_name) 加速爬虫多进程分布式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 倒入模块import multiprocessing as mpimport timefrom urllib.request import urlopen, urljoinfrom bs4 import BeautifulSoupimport rebase_url = \"http://127.0.0.1:4000/\"# base_url = 'https://morvanzhou.github.io/'# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAINif base_url != \"http://127.0.0.1:4000/\": restricted_crawl = Trueelse: restricted_crawl = False#Create a crawl function to open a url in parallel.def crawl(url): response = urlopen(url) #urlopen函数功能 time.sleep(0.1) # slightly delay for downloading return response.read().decode()#Create a parse function to find all results we need in paralleldef parse(html): soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')}) title = soup.find('h1').get_text().strip() page_urls = set([urljoin(base_url, url['href']) for url in urls]) url = soup.find('meta', {'property': \"og:url\"})['content'] return title, page_urls, url#Normal way#Do not use multiprocessing, test the speed. Firstly, set what urls we have already seen and what we haven't in a python set.unseen = set([base_url,])seen = set()count, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visit if restricted_crawl and len(seen) &gt; 20: break print('\\nDistributed Crawling...') htmls = [crawl(url) for url in unseen] print('\\nDistributed Parsing...') results = [parse(html) for html in htmls] print('\\nAnalysing...') seen.update(unseen) # seen the crawled unseen.clear() # nothing unseen for title, page_urls, url in results: print(count, title, url) count += 1 unseen.update(page_urls - seen) # get new url to crawlprint('Total time: %.1f s' % (time.time()-t1, )) # 53 s#multiprocessing#Create a process pool and scrape parallelly.unseen = set([base_url,])seen = set()pool = mp.Pool(4) count, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visit if restricted_crawl and len(seen) &gt; 20: break print('\\nDistributed Crawling...') crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen] htmls = [j.get() for j in crawl_jobs] # request connection print('\\nDistributed Parsing...') parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls] results = [j.get() for j in parse_jobs] # parse html print('\\nAnalysing...') seen.update(unseen) # seen the crawled unseen.clear() # nothing unseen for title, page_urls, url in results: print(count, title, url) count += 1 unseen.update(page_urls - seen) # get new url to crawlprint('Total time: %.1f s' % (time.time()-t1, )) # 16 s !!! 异步加载Asyncio本质是单线程，GIL锁优化，3.5是原生库爬网页用aiohttp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import aiohttpimport asyncioimport timefrom bs4 import BeautifulSoupfrom urllib.request import urljoinimport reimport multiprocessing as mp# base_url = \"https://morvanzhou.github.io/\"base_url = \"http://127.0.0.1:4000/\"# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAINif base_url != \"http://127.0.0.1:4000/\": restricted_crawl = Trueelse: restricted_crawl = False seen = set()unseen = set([base_url])def parse(html): soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')}) title = soup.find('h1').get_text().strip() page_urls = set([urljoin(base_url, url['href']) for url in urls]) url = soup.find('meta', {'property': \"og:url\"})['content'] return title, page_urls, urlasync def crawl(url, session): r = await session.get(url) html = await r.text() await asyncio.sleep(0.1) # slightly delay for downloading return htmlasync def main(loop): pool = mp.Pool(8) # slightly affected async with aiohttp.ClientSession() as session: count = 1 while len(unseen) != 0: print('\\nAsync Crawling...') tasks = [loop.create_task(crawl(url, session)) for url in unseen] finished, unfinished = await asyncio.wait(tasks) htmls = [f.result() for f in finished] print('\\nDistributed Parsing...') parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls] results = [j.get() for j in parse_jobs] print('\\nAnalysing...') seen.update(unseen) unseen.clear() for title, page_urls, url in results: # print(count, title, url) unseen.update(page_urls - seen) count += 1if __name__ == \"__main__\": t1 = time.time() loop = asyncio.get_event_loop() loop.run_until_complete(main(loop)) # loop.close() print(\"Async total time: \", time.time() - t1) 高级爬虫Selenium控制浏览器安装：pip3 install selenium火狐有插件12345678910111213141516171819202122232425262728293031323334353637import osos.makedirs('./img/', exist_ok=True)from selenium import webdriverdriver = webdriver.Chrome()driver.get(\"https://morvanzhou.github.io/\")driver.find_element_by_xpath(u\"//img[@alt='强化学习 (Reinforcement Learning)']\").click()driver.find_element_by_link_text(\"About\").click()driver.find_element_by_link_text(u\"赞助\").click()driver.find_element_by_link_text(u\"教程 ▾\").click()driver.find_element_by_link_text(u\"数据处理 ▾\").click()driver.find_element_by_link_text(u\"网页爬虫\").click()html = driver.page_source # get htmldriver.get_screenshot_as_file(\"./img/sreenshot1.png\")driver.close()print(html[:200])#如果不要浏览器showfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument(\"--headless\") # define headless# add the option when creating driverdriver = webdriver.Chrome(chrome_options=chrome_options) driver.get(\"https://morvanzhou.github.io/\")driver.find_element_by_xpath(u\"//img[@alt='强化学习 (Reinforcement Learning)']\").click()driver.find_element_by_link_text(\"About\").click()driver.find_element_by_link_text(u\"赞助\").click()driver.find_element_by_link_text(u\"教程 ▾\").click()driver.find_element_by_link_text(u\"数据处理 ▾\").click()driver.find_element_by_link_text(u\"网页爬虫\").click()html = driver.page_source # get htmldriver.get_screenshot_as_file(\"./img/sreenshot2.png\")driver.close()print(html[:200]) Scrapy爬虫库 需要拓展是个很大的框架https://www.jianshu.com/p/a8aad3bf4dc4https://blog.csdn.net/u012150179/article/details/32343635123456789101112131415161718192021222324import scrapyclass MofanSpider(scrapy.Spider): name = \"mofan\" start_urls = [ 'https://morvanzhou.github.io/', ] # unseen = set() # seen = set() # we don't need these two as scrapy will deal with them automatically def parse(self, response): yield { # return some results 'title': response.css('h1::text').extract_first(default='Missing').strip().replace('\"', \"\"), 'url': response.url, } urls = response.css('a::attr(href)').re(r'^/.+?/$') # find all sub urls for url in urls: yield response.follow(url, callback=self.parse) # it will filter duplication automatically# lastly, run this in terminal# scrapy runspider 5-2-scrapy.py -o res.json","link":"/2018/10/25/python-爬虫基础/"}],"tags":[{"name":"wordpress","slug":"wordpress","link":"/tags/wordpress/"},{"name":"安全","slug":"安全","link":"/tags/安全/"},{"name":"碎碎念","slug":"碎碎念","link":"/tags/碎碎念/"},{"name":"LAMP","slug":"LAMP","link":"/tags/LAMP/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/技术/"},{"name":"生活","slug":"生活","link":"/categories/生活/"}]}