{"pages":[],"posts":[{"title":"WP的buffer pool报错导致数据库Error的解决方法","text":"首先查看错误日志 12Cd /var/logTail -n 500 mysqld.log 发现错误是： 123456782018-11-11T09:08:20.364026Z 0 [Note] InnoDB: Initializing buffer pool, total size = 50M, instances = 1, chunk size = 50M2018-11-11T09:08:21.751200Z 0 [ERROR] InnoDB: mmap(53690368 bytes) failed; errno 122018-11-11T09:08:21.751262Z 0 [ERROR] InnoDB: Cannot allocate memory for the buffer pool2018-11-11T09:08:21.751274Z 0 [ERROR] InnoDB: Plugin initialization aborted with error Generic error2018-11-11T09:08:22.220815Z 0 [ERROR] Plugin &apos;InnoDB&apos; init function returned error.2018-11-11T09:08:22.526092Z 0 [ERROR] Plugin &apos;InnoDB&apos; registration as a STORAGE ENGINE failed.2018-11-11T09:08:22.526142Z 0 [ERROR] Failed to initialize builtin plugins.2018-11-11T09:08:22.526159Z 0 [ERROR] Aborting 导致的 Wp Error establishing a database connection problem解决方法：改mysql的设置 1234Where is my.cnfCd /etcVim my.cnfinnodb_buffer_pool_size = 50M 依然不行改swap空间空间太小，设置swap分区 参考网址：https://blog.csdn.net/Mr_OOO/article/details/78653523http://junhey.com/2016/06/16/2016-06-17-05-10.htmlhttps://www.cnblogs.com/LUO77/p/5821530.html 参考文件,这儿后面的就是在my.cnf可以直接修改23","link":"/2018/11/14/Wp中数据库Error的解决方法/"},{"title":"“给我做一个类似XX的App”","text":"​ 刚刚看论坛，有人说有客户说到要做一个类似XX的App，当然遭遇的群嘲。无非就是没有具体需求，没有预算种种。Ref：https://www.v2ex.com/t/509918#reply40 ​ 我确实经常遇到这样的客户，我一般会听一听，然后觉得有可行性就做，或者我觉得项目挺好的也做，但是大部分我都不想做或者拒绝了，其实对于小的技术团队和个人来说是浪费精力和竞争力。简单来说就是10年经验，3年技术的现象，你会做的调用的就是那些东西，没啥意思。另一个方面，你一听，坦白说你就大概率知道明年就不能续费了，虽然可能会少很多钱，但是这一锤子买卖确实没的意思。 ​ 基本上创业的人都有一种或多或少改变世界的心，再退一点，就是我的产品不能改变世界，客户的产品牛逼了我也挺开心的。我听说现在做个公司官网就1200块钱还包服务器，有什么做头。 ​ 碎碎念一下，这个野蛮竞争确实没有意思，明年上Ai的生产环境又不知道什么可以落地，CV、NLP还是什么，但这最起码会让觉得有点做头和盼头吧，不然真是苟且生活了。 ​ 在我看来开源，改进轮子都是进步，这个壁垒其实比造轮子高；“做一个类似XX的App”，如果真的完全没有想法、资源，就是给钱无他，就是把客户割韭菜了。 ​ 最后就应该是一种合作共赢的关系，你找我，你的产品可以创造更多，获得更多，其实也就是其实找到好的技术公司应该是客户的幸运，当然我不是说我；我也可以积累行业经验、技术经验，一般这种，我都愿意很少收费或者某种入股来做，不然你直接淘宝做就是了，你那些东西，功能和需求，坦白说，我也做不出花来。 ​ 我自己公司官网写了几句话，“几包泡面，几行代码，一个想法“。就是这样，TF几十行代码就可以做出一个LSNTRNN还带BN云云其实都可以上生产了，你搞个几千行代码吓唬谁和装什么逼呢？今天看抖音有个人新建个文件夹还要终端mkdir，我看你装逼装的真是在家放个屁都要说一声Excuse me。 ​ 忠于生活，忠于理想，改变一点。碎碎念完了，继续看ML，继续想可以怎么实现落地怎么改变。 ​","link":"/2018/11/21/“给我做一个类似XX的App”/"},{"title":"关于打开网页跳流氓广告解决方法","text":"​ 今天GM说我们想做的一个站点打开慢的一逼，然后我打开发现，跳出了4-5个流氓广告，惊了,这不是速度的事情。 大概思路是这么几个： ​ 1、网站被攻击，js注入 ​ 2、浏览器缓存有木马 ​ 3、路由被dns挟持或有后门 方法： 打开很多网站都发现有广告，而且是一样的。排除1 然后清理缓存，重置了浏览器问题依旧。基本判断是问题3 登录路由器，进入dns 江苏联通运营商的dns： 58.240.57.33 221.6.4.66 重新修改密码，重启。 问题解决！","link":"/2018/11/21/关于打开网页跳流氓广告解决方法/"},{"title":"怎么优化LAMP的速度","text":"1、Chrome 访问你的博客，接着 Chrome 的菜单 -》视图 -》开发者 -》开发者工具 -》 Network 刷新一次页面，根据页面加载各种内容的耗时对症下药。 2、服务器延迟，换服务器 3、是否页面 TTFB 过高 动静分离、静态文件未对国内优化、Gravatar 换成国内镜像，CDNJS 换成国内镜像， 静态资源与主站分离 4、php 用 memcached 和 opcache 网络配置换 google 的 bbr 5、 php 执行很慢，你得确认你 php 版本？尽可能上 php7，另外 php 的 APC 缓存加速开了么？ php7 和 apc 缓存 是否开，都直接影响页面执行性能若干倍。 页面和 php 能作的都作了，还是慢，那就得查 db 了。 MySQL 慢查询日志记录开了没？慢查询多不多？ mysql 版本？数据表存储引擎类型？ InnoDB 么？ Innodb mem pool size 够不够大？ phpmyadmin 查查 db 的最近执行性能统计信息。 进系统再查 内存是不是不够，swap 是否频繁读写？ vultr 后台再查查 测试期间 /慢的时候，系统性能 /网络是不是被吃满了？排除资源不够的问题。","link":"/2018/11/19/怎么优化LAMP的速度/"},{"title":"python 爬虫基础","text":"做了一个整理，把python的爬虫基础发一下。这个是基于周莫凡的python整理材料 [TOC] 网页爬虫了解网页结构用python登录网页：123456from urllib.request import urlopen# if has Chinese, apply decode()html = urlopen( \"https://morvanzhou.github.io/static/scraping/basic-structure.html\").read().decode('utf-8')print(html) 然后用正则表达式：12res = re.findall(r\"&lt;p&gt;(.*?)&lt;/p&gt;\", html, flags=re.DOTALL) # re.DOTALL if multi lineprint(\"\\nPage paragraph is: \", res[0]) BeautifulSoup 解析网页BS基础选着要爬的网址 (url)使用 python 登录上这个网址 (urlopen等)读取网页信息 (read() 出来)将读取的信息放入 BeautifulSoup使用 BeautifulSoup 选取 tag 信息等 (代替正则表达式)安装12# Python 3+pip3 install beautifulsoup4 12345678910111213141516from bs4 import BeautifulSoupfrom urllib.request import urlopen# if has Chinese, apply decode()html = urlopen(\"https://morvanzhou.github.io/static/scraping/basic-structure.html\").read().decode('utf-8')print(html)#这边的解析方式有很多 https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ 推荐使用lxmlsoup = BeautifulSoup(html, features='lxml')print(soup.h1) #直接h1print('\\n', soup.p)all_href = soup.find_all('a') #all_href = [l['href'] for l in all_href]print('\\n', all_href) BS 解析CSS12345678910111213141516171819#还是导入两个模块from bs4 import BeautifulSoupfrom urllib.request import urlopen# if has Chinese, apply decode()html = urlopen(\"https://morvanzhou.github.io/static/scraping/list.html\").read().decode('utf-8')print(html)soup = BeautifulSoup(html, features='lxml')# use class to narrow searchmonth = soup.find_all('li', {\"class\": \"month\"})for m in month: print(m.get_text()) #这边用get_text()来获得里面的具体信息jan = soup.find('ul', {\"class\": 'jan'})d_jan = jan.find_all('li') # use jan as a parentfor d in d_jan: print(d.get_text()) BS与正则表达式的运用 导入正则模块 re.compile() 迭代输出,输出的是列表1234567891011121314151617from bs4 import BeautifulSoupfrom urllib.request import urlopenimport re# if has Chinese, apply decode()html = urlopen(\"https://morvanzhou.github.io/static/scraping/table.html\").read().decode('utf-8')print(html)soup = BeautifulSoup(html, features='lxml')img_links = soup.find_all(\"img\", {\"src\": re.compile('.*?\\.jpg')})for link in img_links: print(link['src']) course_links = soup.find_all('a', {'href': re.compile('https://morvan.*')})for link in course_links: print(link['href']) 爬百度百科 重要https://morvanzhou.github.io/tutorials/data-manipulation/scraping/2-04-practice-baidu-baike/12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#Practice: scrape Baidu Baike#Here we build a scraper to crawl Baidu Baike from this page onwards. We store a historical webpage that we have already visited to keep tracking it.#In [2]:from bs4 import BeautifulSoupfrom urllib.request import urlopenimport reimport randombase_url = \"https://baike.baidu.com\"his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]#Select the last sub url in \"his\", print the title and url.url = base_url + his[-1]html = urlopen(url).read().decode('utf-8')soup = BeautifulSoup(html, features='lxml')print(soup.find('h1').get_text(), ' url: ', his[-1])#网络爬虫 url: /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711#Find all sub_urls for baidu baike (item page), randomly select a sub_urls and store it in \"his\". If no valid sub link is found, than pop last url in \"his\".# find valid urlssub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})if len(sub_urls) != 0: his.append(random.sample(sub_urls, 1)[0]['href'])else: # no valid sub link found his.pop()print(his)#['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E4%B8%8B%E8%BD%BD%E8%80%85']#Put everthing together. Random running for 20 iterations. See what we end up with.#his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]for i in range(20): url = base_url + his[-1] html = urlopen(url).read().decode('utf-8') soup = BeautifulSoup(html, features='lxml') print(i, soup.find('h1').get_text(), ' url: ', his[-1]) # find valid urls sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")}) if len(sub_urls) != 0: his.append(random.sample(sub_urls, 1)[0]['href']) else: # no valid sub link found his.pop() 更多请求/下载方式Request的方式Requests：get/post我们就来说两个重要的, get, post, 95% 的时间, 你都是在使用这两个来请求一个网页.post： 账号登录 搜索内容 上传图片 上传文件 往服务器传数据 等get： 正常打开网页 不往服务器传数据 网页使用 get 就可以了, 都是只是 get 发送请求. 而 post, 我们则是给服务器发送个性化请求, 比如将你的账号密码传给服务器, 让它给你返回一个含有你个人信息的 HTML. 从主动和被动的角度来说, post 中文是发送, 比较主动, 你控制了服务器返回的内容. 而 get 中文是取得, 是被动的, 你没有发送给服务器个性化的信息, 它不会根据你个性化的信息返回不一样的 HTML 安装Requests12# python 3+pip3 install requests get有一个参数 params，就是Request.post用Session来传递cookie,就是Session.get注意post的地址，get地址里面没有现实的参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#requests: an alternative to urllib#requests has more functions to replace urlopen. Use request.get() to replace urlopen() and pass some parameters to the webpage. The webbrowser is to open the new url and give you an visualization of this result.import requestsimport webbrowser #内置模块，打开浏览器param = {\"wd\": \"莫烦Python\"}r = requests.get('http://www.baidu.com/s', params=param)print(r.url)webbrowser.open(r.url)http://www.baidu.com/s?wd=%E8%8E%AB%E7%83%A6Python#Out[2]:#True##post##We test the post function in this page. To pass some data to the server to analyse and send some response to you accordingly.data = {'firstname': '莫烦', 'lastname': '周'}r = requests.post('http://pythonscraping.com/files/processing.php', data=data)print(r.text)#Hello there, 莫烦 周!##upload image##We still use post function to update image in this page.file = {'uploadFile': open('./image.png', 'rb')}r = requests.post('http://pythonscraping.com/files/processing2.php', files=file)print(r.text)#The file image.png has been uploaded.#login##Use post method to login to a website.payload = {'username': 'Morvan', 'password': 'password'}r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)print(r.cookies.get_dict())r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)print(r.text)#{'username': 'Morvan', 'loggedin': '1'}#Hey Morvan! Looks like you're still logged into the site!##another general way to login##Use session instead requests. Keep you in a session and keep track the cookies.session = requests.Session()payload = {'username': 'Morvan', 'password': 'password'}r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)print(r.cookies.get_dict())r = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")print(r.text)#{'username': 'Morvan', 'loggedin': '1'}#Hey Morvan! Looks like you're still logged into the site! 下载文件3种方式1234567891011121314151617181920#了下载到一个特定的文件夹, 我们先建立一个文件夹吧. 并且规定这个图片下载地址import osos.makedirs('./img/', exist_ok=True)IMAGE_URL = \"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png\"#方法1 使用 urlretrieve from urllib.request import urlretrieveurlretrieve(IMAGE_URL, './img/image1.png')#方法2 使用 request import requestsr = requests.get(IMAGE_URL)with open('./img/image2.png', 'wb') as f: f.write(r.content) #方法3 使用流的方式，不然只能先存在内存里r = requests.get(IMAGE_URL, stream=True) # stream loadingwith open('./img/image3.png', 'wb') as f: for chunk in r.iter_content(chunk_size=32): f.write(chunk) 循环下载图片requests 访问和 下载功能, 还有 BeautifulSoup、找到img_list下面。。。的img的src 找到图片位置，分析12345678910111213141516171819202122232425from bs4 import BeautifulSoupimport requestsURL = \"http://www.nationalgeographic.com.cn/animals/\"# find list of image holderhtml = requests.get(URL).textsoup = BeautifulSoup(html, 'lxml')img_ul = soup.find_all('ul', {\"class\": \"img_list\"})#Create a folder for these picturesimport osos.makedirs('./img/', exist_ok=True)#download#Find all picture urls and download them.In [4]:for ul in img_ul: imgs = ul.find_all('img')# find_all()函数 for img in imgs: url = img['src'] r = requests.get(url, stream=True) #用流的方式 image_name = url.split('/')[-1] with open('./img/%s' % image_name, 'wb') as f: for chunk in r.iter_content(chunk_size=128): f.write(chunk) print('Saved %s' % image_name) 加速爬虫多进程分布式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 倒入模块import multiprocessing as mpimport timefrom urllib.request import urlopen, urljoinfrom bs4 import BeautifulSoupimport rebase_url = \"http://127.0.0.1:4000/\"# base_url = 'https://morvanzhou.github.io/'# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAINif base_url != \"http://127.0.0.1:4000/\": restricted_crawl = Trueelse: restricted_crawl = False#Create a crawl function to open a url in parallel.def crawl(url): response = urlopen(url) #urlopen函数功能 time.sleep(0.1) # slightly delay for downloading return response.read().decode()#Create a parse function to find all results we need in paralleldef parse(html): soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')}) title = soup.find('h1').get_text().strip() page_urls = set([urljoin(base_url, url['href']) for url in urls]) url = soup.find('meta', {'property': \"og:url\"})['content'] return title, page_urls, url#Normal way#Do not use multiprocessing, test the speed. Firstly, set what urls we have already seen and what we haven't in a python set.unseen = set([base_url,])seen = set()count, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visit if restricted_crawl and len(seen) &gt; 20: break print('\\nDistributed Crawling...') htmls = [crawl(url) for url in unseen] print('\\nDistributed Parsing...') results = [parse(html) for html in htmls] print('\\nAnalysing...') seen.update(unseen) # seen the crawled unseen.clear() # nothing unseen for title, page_urls, url in results: print(count, title, url) count += 1 unseen.update(page_urls - seen) # get new url to crawlprint('Total time: %.1f s' % (time.time()-t1, )) # 53 s#multiprocessing#Create a process pool and scrape parallelly.unseen = set([base_url,])seen = set()pool = mp.Pool(4) count, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visit if restricted_crawl and len(seen) &gt; 20: break print('\\nDistributed Crawling...') crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen] htmls = [j.get() for j in crawl_jobs] # request connection print('\\nDistributed Parsing...') parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls] results = [j.get() for j in parse_jobs] # parse html print('\\nAnalysing...') seen.update(unseen) # seen the crawled unseen.clear() # nothing unseen for title, page_urls, url in results: print(count, title, url) count += 1 unseen.update(page_urls - seen) # get new url to crawlprint('Total time: %.1f s' % (time.time()-t1, )) # 16 s !!! 异步加载Asyncio本质是单线程，GIL锁优化，3.5是原生库爬网页用aiohttp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import aiohttpimport asyncioimport timefrom bs4 import BeautifulSoupfrom urllib.request import urljoinimport reimport multiprocessing as mp# base_url = \"https://morvanzhou.github.io/\"base_url = \"http://127.0.0.1:4000/\"# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAINif base_url != \"http://127.0.0.1:4000/\": restricted_crawl = Trueelse: restricted_crawl = False seen = set()unseen = set([base_url])def parse(html): soup = BeautifulSoup(html, 'lxml') urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')}) title = soup.find('h1').get_text().strip() page_urls = set([urljoin(base_url, url['href']) for url in urls]) url = soup.find('meta', {'property': \"og:url\"})['content'] return title, page_urls, urlasync def crawl(url, session): r = await session.get(url) html = await r.text() await asyncio.sleep(0.1) # slightly delay for downloading return htmlasync def main(loop): pool = mp.Pool(8) # slightly affected async with aiohttp.ClientSession() as session: count = 1 while len(unseen) != 0: print('\\nAsync Crawling...') tasks = [loop.create_task(crawl(url, session)) for url in unseen] finished, unfinished = await asyncio.wait(tasks) htmls = [f.result() for f in finished] print('\\nDistributed Parsing...') parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls] results = [j.get() for j in parse_jobs] print('\\nAnalysing...') seen.update(unseen) unseen.clear() for title, page_urls, url in results: # print(count, title, url) unseen.update(page_urls - seen) count += 1if __name__ == \"__main__\": t1 = time.time() loop = asyncio.get_event_loop() loop.run_until_complete(main(loop)) # loop.close() print(\"Async total time: \", time.time() - t1) 高级爬虫Selenium控制浏览器安装：pip3 install selenium火狐有插件12345678910111213141516171819202122232425262728293031323334353637import osos.makedirs('./img/', exist_ok=True)from selenium import webdriverdriver = webdriver.Chrome()driver.get(\"https://morvanzhou.github.io/\")driver.find_element_by_xpath(u\"//img[@alt='强化学习 (Reinforcement Learning)']\").click()driver.find_element_by_link_text(\"About\").click()driver.find_element_by_link_text(u\"赞助\").click()driver.find_element_by_link_text(u\"教程 ▾\").click()driver.find_element_by_link_text(u\"数据处理 ▾\").click()driver.find_element_by_link_text(u\"网页爬虫\").click()html = driver.page_source # get htmldriver.get_screenshot_as_file(\"./img/sreenshot1.png\")driver.close()print(html[:200])#如果不要浏览器showfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument(\"--headless\") # define headless# add the option when creating driverdriver = webdriver.Chrome(chrome_options=chrome_options) driver.get(\"https://morvanzhou.github.io/\")driver.find_element_by_xpath(u\"//img[@alt='强化学习 (Reinforcement Learning)']\").click()driver.find_element_by_link_text(\"About\").click()driver.find_element_by_link_text(u\"赞助\").click()driver.find_element_by_link_text(u\"教程 ▾\").click()driver.find_element_by_link_text(u\"数据处理 ▾\").click()driver.find_element_by_link_text(u\"网页爬虫\").click()html = driver.page_source # get htmldriver.get_screenshot_as_file(\"./img/sreenshot2.png\")driver.close()print(html[:200]) Scrapy爬虫库 需要拓展是个很大的框架https://www.jianshu.com/p/a8aad3bf4dc4https://blog.csdn.net/u012150179/article/details/32343635123456789101112131415161718192021222324import scrapyclass MofanSpider(scrapy.Spider): name = \"mofan\" start_urls = [ 'https://morvanzhou.github.io/', ] # unseen = set() # seen = set() # we don't need these two as scrapy will deal with them automatically def parse(self, response): yield { # return some results 'title': response.css('h1::text').extract_first(default='Missing').strip().replace('\"', \"\"), 'url': response.url, } urls = response.css('a::attr(href)').re(r'^/.+?/$') # find all sub urls for url in urls: yield response.follow(url, callback=self.parse) # it will filter duplication automatically# lastly, run this in terminal# scrapy runspider 5-2-scrapy.py -o res.json","link":"/2017/10/25/python-爬虫基础/"},{"title":"Tensorflow 学习笔记","text":"[TOC] Tensorflow准备 D1安装 下载地址：https://www.tensorflow.org/install/推荐使用Virtualenv 安装 TensorFlow只用安装某个python的ts每次在新的 shell 中使用 TensorFlow 时，您都必须激活 Virtualenv 环境 斯坦佛：http://web.stanford.edu/class/cs20si/blibli视频：2017https://www.bilibili.com/video/av9156347/?from=search&amp;seid=6905181275544516403youtube：https://www.youtube.com/watch?v=g-EvyKpZjmQ&amp;list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN- 数据集库：收集数据集https://zhuanlan.zhihu.com/p/35399323https://deeplearning4j.org/cn/opendata 版本：TF learnTF SlimHigh level API:Keras 学会使用docker容器：学习python： 概念 import tensorflow as tf本质：产生计算图可视化：tensorboard tensor:0-d:number1-d:vector2-d:matrix 神经网络结构input layer –hidden layer– output layer（拟合数据）怎么处理数据结构： 建立结构 放数据进结构里面 weight 和 baias（权重和偏置） GradientDescentOptimizer 优化器eg12345678910111213141516171819202122232425262728293031import tensorflow as tfimport numpy as np//create datax_dara = np.random.rand(100).astype(np.float32) # ts一般数据都是float32的形式y_data = x_data*0.1 + 0.3### create tensorflow structure start ###Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))# V大写可能是多维矩阵，[1]表示一维，在-1到1位的范围biases = tf.Variable(tf.zeros([1])) # 设置初始值是0y = Weights*x_data + biasesloss = tf.reduce_mean(tf.square(y-y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)# 优化器，有很多optimizer，GradientDescentOptimizer，后面是学习效率。train = optimizer.minimize(loss)### create tensorflow structure end ###sess = tf.Session() //神经网络激活# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)for step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases)) sess.run矩阵乘法123456789101112131415161718from __future__ import print_functionimport tensorflow as tfmatrix1 = tf.constant([[3, 3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1, matrix2) # matrix multiply np.dot(m1, m2) 矩阵乘法# method 1sess = tf.Session() # Session 大写result = sess.run(product)print(result)sess.close()# method 2with tf.Session() as sess: # 自动关闭，在with语句中 result2 = sess.run(product) print(result2) 变量12345678910111213141516171819202122from __future__ import print_functionimport tensorflow as tfstate = tf.Variable(0, name='counter') #定义一个变量，数值和名字#print(state.name)one = tf.constant(1) # 定义一个常量1new_value = tf.add(state, one) # 加法运算update = tf.assign(state, new_value) # tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12 ，最重要的一步if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for _ in range(3): # 做三次循环 sess.run(update) print(sess.run(state)) feeds 和 placeholder 传入值123456789from __future__ import print_functionimport tensorflow as tfinput1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2) #乘法with tf.Session() as sess: print(sess.run(output, feed_dict={input1: [7.], input2: [2.]})) # 这两个绑定，字典形式 激励函数 activation function解决不能用线性方程解决的问题y=Wxy=AF(Wx) 必须可以微分的AF()就是其他的函数，sigmoid，tanh，relu隐藏层只有2-3层，不复杂的时候，任意的都可以；多的要考虑梯度爆炸梯度消失等问题默认：卷积神经网络：relu ；循环神经网络：relu or tanh 方程：线性，阶梯（-1不激活，1为激励） 非线性：接近于0，或者1（分类问题）tf在layer2层多数，看是否要激活或者不激活 googletensorflow activation查看有哪些activation 添加定义神经层 def12345678910111213from __future__ import print_functionimport tensorflow as tfdef add_layer(inputs, in_size, out_size, activation_function=None): #添加一个层，传入数据 Weights = tf.Variable(tf.random_normal([in_size, out_size])) #定义矩阵就大写，in_size和out_size 的随机矩阵 biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) #1行，out_size列，推荐不为0 Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: #就是一个线性函数，就不用加层 outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 建造一个神经网络12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from __future__ import print_functionimport tensorflow as tfimport numpy as np #用到numpydef add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real data x_data = np.linspace(-1,1,300)[:, np.newaxis] #-1-1区间有300个单位，300行，加一个维度noise = np.random.normal(0, 0.05, x_data.shape) # 加入噪点，不完全拟合，方差是0.05，和xdata一个格式y_data = np.square(x_data) - 0.5 + noise #用nonliner，二次方# define placeholder for inputs to network xs = tf.placeholder(tf.float32, [None, 1]) ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) #in_size=1，，一个参数，output_size是10，就是输出10个神经元，激活函数。# add output layer 隐藏层的输出值，输入10个，输出1个。没有激活函数prediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediction and real data 平方差公式tf.square，然后求和tf.reduce_sum，平均误差tf.reduce_mean，reduction_indices=[1]定义轴，学习率是0.1，loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for i in range(1000): # training sess.run(train_step, feed_dict={xs: x_data, ys: y_data}) if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict={xs: x_data, ys: y_data})) 结果可视化 ？？？123456789101112131415161718192021import matplotlib.pyplot as plt# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion()plt.show()for i in range(1000): # training sess.run(train_step, feed_dict={xs: x_data, ys: y_data}) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) except Exception: pass prediction_value = sess.run(prediction, feed_dict={xs: x_data}) # plot the prediction lines = ax.plot(x_data, prediction_value, 'r-', lw=5) plt.pause(0.1) SGD stochastic gradient descent每次使用批量数据MementumAdagradRMSProp 两者合成Adam 方法最好 Optimizer 优化器https://www.tensorflow.org/api_guides/python/train 可视化的好帮手 TensorboardTensorboardwith tf.name_scope(‘layer’):1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from __future__ import print_functionimport tensorflow as tfdef add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer with tf.name_scope('layer'): with tf.name_scope('weights'): Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W') with tf.name_scope('biases'): biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b') with tf.name_scope('Wx_plus_b'): Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs# define placeholder for inputs to networkwith tf.name_scope('inputs'): xs = tf.placeholder(tf.float32, [None, 1], name='x_input') ys = tf.placeholder(tf.float32, [None, 1], name='y_input')# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediciton and real datawith tf.name_scope('loss'): loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))with tf.name_scope('train'): train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)sess = tf.Session()# tf.train.SummaryWriter soon be deprecated, use followingif int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: # tensorflow version &lt; 0.12 writer = tf.train.SummaryWriter('logs/', sess.graph)else: # tensorflow version &gt;= 0.12 writer = tf.summary.FileWriter(\"logs/\", sess.graph)# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)# direct to the local dir and run this in terminal:# $ tensorboard --logdir=logs 分类学习 MNIST123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from __future__ import print_functionimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# number 1 to 10 datamnist = input_data.read_data_sets('MNIST_data', one_hot=True)def add_layer(inputs, in_size, out_size, activation_function=None,): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1,) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b,) return outputsdef compute_accuracy(v_xs, v_ys): global prediction y_pre = sess.run(prediction, feed_dict={xs: v_xs}) correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys}) return result# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 784]) # 28x28ys = tf.placeholder(tf.float32, [None, 10])# add output layerprediction = add_layer(xs, 784, 10, activation_function=tf.nn.softmax)# the error between prediction and real data cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) # losstrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)sess = tf.Session()# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys}) if i % 50 == 0: print(compute_accuracy( mnist.test.images, mnist.test.labels)) 过度拟合 增加数据量 L1，L2正规化 dropout 过度拟合的dropoutref:Sklearn:https://morvanzhou.github.io/tutorials/machine-learning/sklearn/1-1-A-ML/12345678910111213141516171819# here to dropout 50%舍弃掉Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)if activation_function is None: outputs = Wx_plus_belse: outputs = activation_function(Wx_plus_b, )tf.summary.histogram(layer_name + '/outputs', outputs)return outputs# define placeholder for inputs to networkkeep_prob = tf.placeholder(tf.float32)xs = tf.placeholder(tf.float32, [None, 64]) # 8x8ys = tf.placeholder(tf.float32, [None, 10])# here to determine the keeping probabilitysess.run(train_step, feed_dict={xs: X_train, ys: y_train, keep_prob: 0.5}) #一定要有histogram_summary CNN 卷积神经网络基本原理alpha go卷积神经网络有一个批量过滤器, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理, 这时候整理出来的信息有了一些实际上的呈现。图片是如何被卷积的. 下面是一张猫的图片, 图片有长, 宽, 高 三个参数. 图片是有高度的! 这里的高指的是计算机用于产生颜色使用的信息. 如果是黑白照片的话, 高的单位就只有1, 如果是彩色照片, 就可能有红绿蓝三种颜色的信息, 这时的高度为3。 将图片的长宽再压缩, 高度再增加, 就有了对输入图片更深的理解。 池化（pooling）：在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题。 流行的CNN结构： CNN进阶google 自己的CNN介绍不断压缩——————运用厚度信息变成一个分类器（classifier）抽离参数：stride（几个像素点）在patch（kernal）里面方式是padding 两种方式另外就是pooling 也分为两种 CNN代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788\"\"\"Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.\"\"\"from __future__ import print_functionimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# number 1 to 10 datamnist = input_data.read_data_sets('MNIST_data', one_hot=True)def compute_accuracy(v_xs, v_ys): global prediction y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1}) correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1}) return resultdef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) #tf.truncted_normal产生随机变量来进行初始化 return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape)#tf.constant常量函数来进行初始化,初始值是0.1，正值比较好，然后传参。 return tf.Variable(initial)def conv2d(x, W):#定义卷积，x是输入值（图片），W是上面的weight # stride [1, x_movement, y_movement, 1] # Must have strides[0] = strides[3] = 1，步长第一和第四个都是1，x是1，y也是1； # 二维的tf.nn.conv2d函数是tensoflow里面的二维的卷积函数，padding,一种是valid（抽取是全部图片里面的），SAME有部分抽取 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): # stride [1, x_movement, y_movement, 1]，两种方法：max，average，相当于压缩了，因为把图片压缩了，不用传入参数，其他和conv2d类似 return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 784])/255. # 28x28ys = tf.placeholder(tf.float32, [None, 10])keep_prob = tf.placeholder(tf.float32)x_image = tf.reshape(xs, [-1, 28, 28, 1]) #传入层之前需要改下形状，-1代表先不考虑输入的图片例子多少这个维度，后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1，例如如果是RGB图像，那么channel就是3。# print(x_image.shape) # [n_samples, 28,28,1]## conv1 layer ## 定义卷积层1W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32 ，提取5*5像素的图片，输入1个像素的单位，输出32个像素的色彩高度b_conv1 = bias_variable([32])#32个长度h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32 same方式长款不变还是28，高度变成了32h_pool1 = max_pool_2x2(h_conv1) # output size 14x14x32 ，就是28/2，因为pooling的时候步长多了1倍，图片小了1倍## conv2 layer ##W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64 ，传入32，传出变成64b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64h_pool2 = max_pool_2x2(h_conv2) # output size 7x7x64## fc1 layer ## 建立全联接层W_fc1 = weight_variable([7*7*64, 1024]) #输入的是，输出1024的高度，变得更高b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])#变平，先不管多少个样品，由立方体变成扁平，[n_samples, 7, 7, 64] -&gt;&gt; [n_samples, 7*7*64]，改形状h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)#做矩阵的乘法h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)#做一个dropout的处理，防止过度拟合的情况## fc2 layer ##W_fc2 = weight_variable([1024, 10])#输出结果是10位的b_fc2 = bias_variable([10])prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)#用softmax做分类处理，算概率# the error between prediction and real datacross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) # losstrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)#庞大系统，不用grient的优化器，选一个更小的学习参数sess = tf.Session()# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5}) if i % 50 == 0: print(compute_accuracy( mnist.test.images[:1000], mnist.test.labels[:1000])) saver 保存和读取最后定义dtype以float32的格式，和名字name12345678910111213141516171819202122232425262728293031323334353637from __future__ import print_functionimport tensorflow as tfimport numpy as np# Save to file# remember to define the same dtype and shape when restore# W = tf.Variable([[1,2,3],[3,4,5]], dtype=tf.float32, name='weights')# b = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases')# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12# if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1:# init = tf.initialize_all_variables()# else:# init = tf.global_variables_initializer()## saver = tf.train.Saver()## with tf.Session() as sess:# sess.run(init)# save_path = saver.save(sess, \"my_net/save_net.ckpt\") #后面是路径，格式是ckpt# print(\"Save to path: \", save_path)################################################# restore variables 提取变量# redefine the same shape and same type for your variables ，还需要重新的定义，数据类型和形状是一样的要，上面是6个数据，然后形状一个（2，3）向量W = tf.Variable(np.arange(6).reshape((2, 3)), dtype=tf.float32, name=\"weights\")b = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name=\"biases\")# not need init stepsaver = tf.train.Saver()with tf.Session() as sess: saver.restore(sess, \"my_net/save_net.ckpt\")#后面是路径 print(\"weights:\", sess.run(W)) print(\"biases:\", sess.run(b)) RNN 循环神经网络概念st 用来影响 st1时刻，确定yn+1，有顺序的就可以用RNN，CNN用的滤波器的量是同一个，只不过rnn有时间顺序上有，eg：https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-07-B-LSTM/Tensorflow PyTorch Keras Cell：RNN中的滤波器叫cell，区别在于有部分存储。然后输出y2（考量的不仅仅是x2，还有y1）state：上一步的结果叫state，然后输入x2，产生新的state。 更先进的：上面可能参数1.1的n次方导致梯度爆炸，LSTM RNN（深度学习）解决梯度爆炸消失，多三个控制器，多了一个gate，要不要记住这个点，输出的时候要不要读取，要不要忘记state，就是这个state要不要进入主线。 long short term memory： 代码实现（分类例子，mnist）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# set random seed for comparing the two result calculationstf.set_random_seed(1)# this is datamnist = input_data.read_data_sets('MNIST_data', one_hot=True)# hyperparameterslr = 0.001 #学习率training_iters = 100000 #循环次数batch_size = 128 #自己定的 每次池子里面拿128个数据n_inputs = 28 # MNIST data input (img shape: 28*28) ，每一次输入一行的像素n_steps = 28 # time steps 有28行，输入28步n_hidden_units = 128 # neurons in hidden layer n_classes = 10 # MNIST classes (0-9 digits) 分成10个类，0-9# tf Graph inputx = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.float32, [None, n_classes])# Define weightsweights = { # (28, 128) 'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])), # (128, 10) 'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))}biases = { # (128, ) 'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])), # (10, ) 'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))}def RNN(X, weights, biases): # hidden layer for input to cell ######################################## # transpose the inputs shape from 定义传进来的 X ==&gt; (128 batch * 28 steps, 28 inputs)，转换成-1总起来，原始的 X 是 3 维数据, 我们需要把它变成 2 维数据才能使用 weights 的矩阵乘法 X = tf.reshape(X, [-1, n_inputs]) # into hidden # X_in = (128 batch * 28 steps, 输出成128 hidden) X_in = tf.matmul(X, weights['in']) + biases['in'] # X_in ==&gt; (变成一个三维数据，128 batch, 28 steps, 128 hidden) X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units]) # cell ########################################## # basic LSTM Cell.有很多种cell，这边用BasicLSTMCell。初始不忘记为1。dynamic_rnn效果更好，state_is_tuple=True分成主线state（就是cstate）和分线mstate，这个是不是主线的，所以选择true if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True) else: cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units) # lstm cell is divided into two parts (c_state, h_state) init_state = cell.zero_state(batch_size, dtype=tf.float32) # You have 2 options for following step. # 1: tf.nn.rnn(cell, inputs); # 2: tf.nn.dynamic_rnn(cell, inputs). # If use option 1, you have to modified the shape of X_in, go and check out this: # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py # In here, we go for option 2. # dynamic_rnn receive Tensor (batch, steps, inputs) or (steps, batch, inputs) as X_in. # Make sure the time_major is changed accordingly.outputs是个list，输出两个结果，time_major=False（时间是不是主要第一纬度，这边steps是第二个位置） outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False) # hidden layer for output as the final results ############################################# # results = tf.matmul(final_state[1], weights['out']) + biases['out'] # # or # unpack to list [(batch, outputs)..] * steps if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: outputs = tf.unpack(tf.transpose(outputs, [1, 0, 2])) # states is the last outputs else: outputs = tf.unstack(tf.transpose(outputs, [1,0,2])) results = tf.matmul(outputs[-1], weights['out']) + biases['out'] # shape = (128, 10) return resultspred = RNN(x, weights, biases)cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))train_op = tf.train.AdamOptimizer(lr).minimize(cost)correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))with tf.Session() as sess: # tf.initialize_all_variables() no long valid from # 2017-03-02 if using tensorflow &gt;= 0.12 if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables() else: init = tf.global_variables_initializer() sess.run(init) step = 0 while step * batch_size &lt; training_iters: batch_xs, batch_ys = mnist.train.next_batch(batch_size) batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs]) sess.run([train_op], feed_dict={ x: batch_xs, y: batch_ys, }) if step % 20 == 0: print(sess.run(accuracy, feed_dict={ x: batch_xs, y: batch_ys, })) step += 1 Rnn 回归例子基于tf0.10123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160# View more python learning tutorial on my Youtube and Youku channel!!!# Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg# Youku video tutorial: http://i.youku.com/pythontutorial\"\"\"Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.Run this script on tensorflow r0.10. Errors appear when using lower versions.\"\"\"import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltBATCH_START = 0TIME_STEPS = 20BATCH_SIZE = 50INPUT_SIZE = 1OUTPUT_SIZE = 1CELL_SIZE = 10LR = 0.006def get_batch(): #生成数据的function global BATCH_START, TIME_STEPS # xs shape (50batch, 20steps) xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi) seq = np.sin(xs) res = np.cos(xs) BATCH_START += TIME_STEPS # plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--') # plt.show() # returned seq, res and xs: shape (batch, step, input) return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]class LSTMRNN(object): def __init__(self, n_steps, input_size, output_size, cell_size, batch_size): self.n_steps = n_steps self.input_size = input_size self.output_size = output_size self.cell_size = cell_size self.batch_size = batch_size with tf.name_scope('inputs'): self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs') self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys') with tf.variable_scope('in_hidden'): self.add_input_layer() with tf.variable_scope('LSTM_cell'): self.add_cell() with tf.variable_scope('out_hidden'): self.add_output_layer() with tf.name_scope('cost'): self.compute_cost() with tf.name_scope('train'): self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost) def add_input_layer(self,): l_in_x = tf.reshape(self.xs, [-1, self.input_size], name='2_2D') # (batch*n_step, in_size，把三维数据改成二维) # Ws (in_size, cell_size) Ws_in = self._weight_variable([self.input_size, self.cell_size]) # bs (cell_size, ) bs_in = self._bias_variable([self.cell_size,]) # l_in_y = (batch * n_steps, cell_size) with tf.name_scope('Wx_plus_b'): l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in # reshape l_in_y ==&gt; (batch, n_steps, cell_size)，再转化成3d self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D') def add_cell(self): lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True) with tf.name_scope('initial_state'): self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32) self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn( lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False) def add_output_layer(self): # shape = (batch * steps, cell_size) l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D') Ws_out = self._weight_variable([self.cell_size, self.output_size]) bs_out = self._bias_variable([self.output_size, ]) # shape = (batch * steps, output_size) with tf.name_scope('Wx_plus_b'): self.pred = tf.matmul(l_out_x, Ws_out) + bs_out def compute_cost(self): losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example( [tf.reshape(self.pred, [-1], name='reshape_pred')], [tf.reshape(self.ys, [-1], name='reshape_target')], [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)], average_across_timesteps=True, softmax_loss_function=self.ms_error, name='losses' ) with tf.name_scope('average_cost'): self.cost = tf.div( tf.reduce_sum(losses, name='losses_sum'), self.batch_size, name='average_cost') tf.summary.scalar('cost', self.cost) @staticmethod def ms_error(labels, logits): return tf.square(tf.subtract(labels, logits)) def _weight_variable(self, shape, name='weights'): initializer = tf.random_normal_initializer(mean=0., stddev=1.,) return tf.get_variable(shape=shape, initializer=initializer, name=name) def _bias_variable(self, shape, name='biases'): initializer = tf.constant_initializer(0.1) return tf.get_variable(name=name, shape=shape, initializer=initializer)if __name__ == '__main__': model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE) sess = tf.Session() merged = tf.summary.merge_all() writer = tf.summary.FileWriter(\"logs\", sess.graph) # tf.initialize_all_variables() no long valid from # 2017-03-02 if using tensorflow &gt;= 0.12 if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables() else: init = tf.global_variables_initializer() sess.run(init) # relocate to the local dir and run this line to view it on Chrome (http://0.0.0.0:6006/): # $ tensorboard --logdir='logs' plt.ion() plt.show() for i in range(200): seq, res, xs = get_batch() if i == 0: feed_dict = { model.xs: seq, model.ys: res, # create initial state } else: feed_dict = { model.xs: seq, model.ys: res, model.cell_init_state: state # use last state as the initial state for this run } _, cost, state, pred = sess.run( [model.train_op, model.cost, model.cell_final_state, model.pred], feed_dict=feed_dict) # plotting plt.plot(xs[0, :], res[0].flatten(), 'r', xs[0, :], pred.flatten()[:TIME_STEPS], 'b--') plt.ylim((-1.2, 1.2)) plt.draw() plt.pause(0.3) if i % 20 == 0: print('cost: ', round(cost, 4)) result = sess.run(merged, feed_dict) writer.add_summary(result, i) 自编码（非监督学习） autoencoderX——原数据精髓——黑的X编码器能得到原数据的精髓, 然后我们只需要再创建一个小的神经网络学习这个精髓的数据,不仅减少了神经网络的负担, 而且同样能达到很好的效果.PCA ？？？ https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf21_autoencoder/full_code.py scope??? Batch Normalization 批标准化迁移学习VGG的CV 16层直接用别人训练好的一部分CNN，参数可以固定https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-16-transfer-learning/","link":"/2018/03/25/Tensorflow-学习笔记/"},{"title":"python 文档","text":"[TOC] 这个是基于莫凡python，廖雪峰的自学整理 安装安装python31brew install python3 选择python3 —versionatom 安装 scriptcmd shift +p 查找 run 快捷键cmd i打开setting：cmd + ，1pip3 install jupyter 这边用conda更方便 基础功能print 功能Print 功能： Print(1) 3.5以上一定要有括号 Print string的话 用单引号或者双引号，双引号里面加单引号 \\是系统识别符号，数字不可以加字符串 坚持使用4个空格的缩进大小写敏感的平方是1234568%3是取余数，9//4取整```pythonprint(int(&apos;2&apos;)+3) #int为定义整数型print(int(1.9)) #当int一个浮点型数时，int会保留整数部分print(float(&apos;1.2&apos;)+3) #float()是浮点型，可以把字符串转换成小数 格式化用%实现 %s表示用字符串替换 %d表示用整数替换 有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个%?，括号可以省略。 %s永远起作用%s 字符串 (采用str()的显示)%r 字符串 (采用repr()的显示)%c 单个字符%b 二进制整数%d 十进制整数%i 十进制整数%o 八进制整数%x 十六进制整数%e 指数 (基底写为e)%E 指数 (基底写为E)%f 浮点数%F 浮点数，与上相同%g 指数(e)或浮点数 (根据显示长度)%G 指数(E)或浮点数 (根据显示长度)%% 字符”%”123print ('Hi, %s, you have $%d.' % ('Michael', 1000000))print( 'growth rate: %d %%' % 7)print( 'Age: %s. Gender: %s' % (25, True)) 定义变量123apple=1apple_2016='iphone 7 plus'a,b,c=1,2,3 #定义多个自变量 循环条件判断注意不要少写了冒号:1234567age = 3if age &gt;= 18: print('adult')elif age &gt;= 6: print('teenager')else: print(‘kid') // // while循环循环是while循环，只要条件满足，就不断循环，条件不满足时退出循环。如果要提前结束循环，可以用break语句1234567n = 1while n &lt;= 100: if n &gt; 10: # 当n = 11时，条件满足，执行break语句 break # break语句会结束当前循环 print(n) n = n + 1print('END') continue和break，pass123456n = 0while n &lt; 10: n = n + 1 if n % 2 == 0: # 如果n是偶数，执行continue语句 continue # continue语句会直接继续下一轮循环，后续的print()语句不会执行 print(n) for循环for…in…循环作为迭代器,cmd+[改变tab缩进结构依次把list或tuple中的每个元素迭代出来，迭代器的作用，看例子：123names = ['Michael', 'Bob', 'Tracy']for name in names: print(name) range是自定义函数，range(1,10)其实是1-91234sum = 0for x in range(101): # 到100 sum = sum + xprint(sum) if判断python 语言中等号的判断使用 == 而不是 =, 因为后一种是赋值语句要有冒号，&lt;,&gt;.&lt;=,&lt;=,==（等于）,!=(不等于), 12345x = 1y = 2z = 3if x &lt; y: print('x is less than y') if…else…同等关系，都有冒号1234if condition: true_expressionselse: false_expressions 1234567x = 1y = 2z = 3if x &gt; y: print('x is greater than y')else: print('x is less or equal to y') if..elif…else12345678910if condition1: true1_expressionselif condition2: true2_expressionselif condtion3: true3_expressionselif ... ...else: else_expressions 12345678910x = 4y = 2z = 3if x &gt; 1: print ('x &gt; 1')elif x &lt; 1: print('x &lt; 1')else: print('x = 1')print('finish') 定义功能定义函数四个空格12def function_name(parameters): expressions 函数参数123def fun1(a,b): c=a*b print('the c is ',c) 函数的默认参数没有的放前面，后面有默认的放后面，这样就可以直接输出了12345def sale_car(price, color='red', brand='carmy', is_second_hand=True): print('price', price, 'color', color, 'brand', brand, 'is_second_hand', is_second_hand,) 可变参数（可变参数在函数定义不能出现在特定参数和默认参数前面）和关键词参数（参数在函数内部自动封装成一个字典(dict)）1universal_func(*args, **kw) 定义变量局部变量 和 全局变量全局通常全部大写，局部变量小写在函数内；如果在方程中123a=Nonedef fun() global a=30 模块安装用别人好的外部模块加载，比如numpy123pip install numpy # 这是 python2+ 版本的用法pip3 install numpy # 这是 python3+ 版本的用法pip3 install -U numpy # 这是 python3+ 版本的用法 文件读写 就是存模块、文件，下一次再用；\\n 是换行命令，\\t 对齐12text='This is my first test.\\n\\tThis is the second line.\\n\\tThis the third line'print(text) # 输入换行命令\\n，要注意斜杆的方向。 open读文件方式使用 open 能够打开一个文件, open 的第一个参数为文件名和路径 ‘my file.txt’, 第二个参数为将要以什么方式打开它, 比如 w 为可写方式. 如果计算机没有找到 ‘my file.txt’ 这个文件, w 方式能够创建一个新的文件, 并命名为 my file.txt123my_file=open('my file.txt','w') #用法: open('文件名','形式'), 其中形式有'w':write;'r':read.my_file.write(text) #该语句会写入先前定义好的 textmy_file.close() #关闭文件 打开文件要close 给文件添加内容写文件是用，’w’;追加用’a’ 1234append_text='\\nThis is appended file.' # 为这行文字提前空行 \"\\n\"my_file=open('my file.txt','a') # 'a'=append 以增加内容的形式打开my_file.write(append_text)my_file.close() 读文件，是 r使用 file.read() 能够读取到文本的所有内容. 123file= open('my file.txt','r') content=file.read() print(content) 按行读取 file.readline()，readlines就是多行输出12345file= open('my file.txt','r') content=file.readline() # 读取第一行second_read_time=file.readline() # 读取第二行print(content)print(second_read_time) 类 Class 建议首字母大写 冒号不能缺失 运行时class要加（） def第一个参数要是self1234567891011121314class Calculator: #首字母要大写，冒号不能缺 name='Good Calculator' #该行为class的属性 price=18 def add(self,x,y): print(self.name) result = x + y print(result) def minus(self,x,y): result=x-y print(result) def times(self,x,y): print(x*y) def divide(self,x,y): print(x/y) 运行： cal=Calculator()cal.add(10,20) class的init功能其实 __init__可以理解成初始化class的变量，取自英文中initial 最初的意思.可以在运行时，给初始值附值 还是要有self,必须要有参数，不论是先输入的还是默认的 123456789class Calculator: name='good calculator' price=18 def __init__(self,name,price,hight=10,width=14,weight=16): #后面三个属性设置默认值,查看运行 self.name=name self.price=price self.h=hight self.wi=width self.we=weight input模块variable=input() 表示运行后，可以在屏幕中输入一个数字，该数字会赋值给自变量。12a_input=input('please input a number:')print('this number is:',a_input) 这儿可以用在if里面，但是在if语句中自变量 a_input 对应的是1 and 2 整数型。输入的内容和判断句中对应的内容格式应该一致。不然返回值是string 所以要是int，或者就是’1’,或者str（1），不然就是int(input(‘plz give a num’))1234567a_input=int(input('please input a number:'))#注意这里要定义一个整数型if a_input==1: print('This is a good one')elif a_input==2: print('See you next time')else: print('Good luck') 元组（tuple） 列表（List）两个都是有次序的数字turple 小括号list 中括号123a_tuple = (12, 3, 5, 15 , 6)another_tuple = 12, 3, 5, 15 , 6 #tuplea_list = [12, 3, 67, 7, 82] #list 用作迭代，或者定位 参加for..in..循环依次输出a_tuple和a_list中的各个元素,这儿的range，还是迭代器，是4，从0开始12for index in range(len(a_list)): print(\"index = \", index, \", number in list = \", a_list[index]) Listlist的添加方法：XX.append(‘str’),在最后添加一个​ 指定位置: xx.insert(位置，‘项目’)；​ 删除：xx.remove(元素)，会删掉第一个出现的元素，后面会保留指数[0]:print(a[0]) :打印出a列表的第0位 a[-1]:从最后面开始算起 a[0:3]:从第0位到第2位，0，1，2，等同于[:3],a[3:5]输出3，4；​ 还有[5:], [-3:],都是前闭后开a.index(2):第一次出现2（value）的索引a.count(2):算出value出现的次数a.sort():函数排序，从小到大并覆盖 ​1234567classmates.append('Adam')classmates.insert(1, 'Jack') #在第一位添加jacka = [4,1,2,3,4,1,1,-1]a.remove(2) a.sort() # 默认从小到大排序a.sort(reverse=True) # 从大到小排序print(a) tuple和list非常类似，但是tuple一旦初始化就不能修改,“指向不变”. dictionary 类型字典没有顺序，字典每一key对应一个value,key可以是int可以str 赋值 输出位置 删除 储存类型：字典里面再加字典，也可是fun（）12345678910111213141516171819202122dic = {'apple':1,'pear':2}d={1:'a',2:'b'}dic['lan'] = 'python'dic['version'] = 2.7for key in dic: print(key, dic[key]) a_list = [1,2,3,4,5,6,7,8]print(a_list[0]) # 1列表 d1 = {'apple':1, 'pear':2, 'orange':3}print(d1['apple']) # 1字典 输出del d1['pear'] #删除d1['platform'] = 64 #赋值def func(): return 0d4 = {'apple':[1,2,3], 'pear':{1:3, 3:'a'}, 'orange':func}print(d4['pear'][3]) # a set 类型找不同，不需要value，可以只有key，没有序列不能传列表+字符串 12345678910s = set(['python', 'python2', 'python3','python'])for item in s: print(item) char_list = ['a', 'b', 'c', 'c', 'd', 'd', 'd']sentence = 'Welcome Back to This Tutorial' print(set(char_list))print(set(sentence))print(set(char_list+ list(sentence))) 添加：1234unique_char = set(char_list)unique_char.add('x')# unique_char.add(['y', 'z']) this is wrong,不能加一个list和重复的数字print(unique_char) 清除一个元素可以用 remove 或者 discard, 而清除全部可以用 clear.123unique_char.remove('x') #这个返回值就是noneunique_char.discard('d')unique_char.clear() 筛选：.difference .intersection123unique_char = set(char_list)print(unique_char.difference({'a', 'e', 'i'})) #不同print(unique_char.intersection({'a', 'e', 'i'})) Python 共内置了 list、 tuple 、dict 和 set 四种基本集合，每个集合对象都能够迭代。dist和set名值对请务必注意，dict内部存放的顺序和key放入的顺序是没有关系的。需要牢记的第一条就是dict的key必须是不可变对象和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key 多维列表numpy 和 pandas来处理多维，两个括号12345a = [1,2,3,4,5] # 一行五列multi_dim_a = [[1,2,3], [2,3,4], [3,4,5]] # 三行三列 print(multi_dim_a[0][1]) 第0行第1列，就是输出2，都是从0还是索引的 import 一个模块方法112import timeprint(time.localtime()) #这样就可以print 当地时间 方法212import time as tprint(t.localtime()) # 需要加t.前缀来引出功能 方法三：from time import time,localtime ,只import自己想要的功能.所用功能就是，可以省略模块名。12345from time import time, localtimeprint(localtime())print(time())from time import * 自己的模块保存好模块 xx.py,在同一个目录下即可，在Mac系统中，下载的python模块会被存储到外部路径site-packages，同样，我们自己建的模块也可以放到这个路径，最后不会影响到自建模块的调用。（Library-framework-lib-python-….）1import balance 其他try的错误处理处理错误输出错误的方法：try: , except … as …:123456789101112try: file=open('eeee.txt','r+') #这边是读取加写入的操作except Exception as e: print(e) response = input('do you want to create a new file:') if response=='y': file=open('eeee.txt','w') else: passelse: file.write('ssss') file.close() zip lambda mapzip函数接受任意多个（包括0个和1个）序列作为参数，合并后返回一个tuple列表.需要list，这个是数项合并123456a=[1,2,3]b=[4,5,6]ab=zip(a,b)print(list(ab)) #需要加list来可视化这个功能for i,j in zip(a,b): #包括迭代器功能 print(i/2,j*2) lambda定义一个简单的函数，实现简化代码的功能，看代码会更好理解。 1234fun= lambda x,y:x+yx=int(input('x=')) #这里要定义int整数，否则会默认为字符串y=int(input('y='))print(fun(x,y)) mapmap是把函数和参数绑定在一起。12345678910def fun(x,y): return (x+y)list(map(fun,[1],[2])) #这边要list这个函数运算\"\"\"[3]\"\"\"list(map(fun,[1,2],[3,4]))\"\"\"[4,6]\"\"\" copy &amp; deepcopy 浅复制 &amp; 深复制copy是一个模块深拷贝：改变一个变量，另一个也改变，他们指针位置一样，内存空间改变。浅拷贝：第一层列表是完全指定到另外的内存空间，第二层列表旨在在同一个空间，此时第二层可以变，第一层不变。a=b：内存空间不变。123456789101112131415161718192021import copya = [1,2,3]b = ab[1]=22print(a)print(id(a) == id(b)) #True# deep copyc = copy.deepcopy(a)print(id(a) == id(c)) #falsec[1] = 2print(a) #[1,2,3]a[1] = 111 #[1,2,3]print(c)# shallow copya = [1,2,[3,4]]d = copy.copy(a)print(id(a) == id(d)) #false 改变，第一层内存改变print(id(a[2]) == id(d[2])) #true 第二层不变，包括内存 多线程和多进程threading 和 multiprocessing多线程 Threading 是一种让程序拥有分身效果. 能同时处理多件事情. 一般的程序只能从上到下一行行执行代码, 不过 多线程 (Threading) 就能打破这种限制. 让你的程序鲜活起来. 我们在多线程 (Threading) 里提到过, 它是有劣势的, GIL 让它没能更有效率的处理一些分摊的任务. 而现在的电脑大部分配备了多核处理器, 多进程 Multiprocessing 能让电脑更有效率的分配任务给每一个处理器, 这种做法解决了多线程的弊端. 也能很好的提升效率. 多线程添加线程threading加载thereading模块几个函数： threading.active.acount() ,有几个线程 threading.enumerate(),查看所有线程信息 threading.current_thread(),正在运行的线程 12345678910111213141516import threading#def main():# print(threading.active_count()) # print(threading.enumerate()) # see the thread list# print(threading.current_thread())def thread_job(): #定义线程的工作 print('This is a thread of %s' % threading.current_thread())def main(): thread = threading.Thread(target=thread_job,) #T大写，添加多线程 thread.start() #开始线程if __name__ == '__main__': main() join功能线程任务还未完成便输出all done。如果要遵循顺序，可以在启动线程后对它调用join12345678910111213141516171819import threadingimport timedef T1_job(): print(\"T1 start\\n\") for i in range(10): time.sleep(0.1) print(\"T1 finish\\n\")def T2_job(): print(\"T2 start\\n\") print(\"T2 finish\\n\")thread_1 = threading.Thread(target=T1_job, name='T1')thread_2 = threading.Thread(target=T2_job, name='T2')thread_1.start() # 开启T1thread_2.start() # 开启T2thread_2.join() # join for T2thread_1.join() # join for T1print(\"all done\\n\") 储存进程结果Queue多线程没有结果，将结果保存在Queue中，线程执行完后，从Queue中获取存储的结果1234567891011121314151617181920212223242526import threadingimport timefrom queue import Queue #定义一个模块，在小写q模块导入大写Qdef job(l,q): for i in range(len(l)): #这边的l是一个列表，len(l)就是列表的长度，即个数 l[i] = l[i]**2 #列表的值的平方 q.put(l) #多线程调用的函数不能用return返回值def multithreading(): q = Queue() #q中存放返回值，代替return的返回值 threads = [] #线程列表 data = [[1,2,3],[3,4,5],[4,4,4],[5,5,5]] for i in range(4): : #定义四个线程 t = threading.Thread(target=job, args=(data[i], q)) #Thread首字母要大写，被调用的job函数没有括号，只是一个索引，参数在后面 t.start() #开始线程 threads.append(t) #把每个线程append到线程列表中 for thread in threads: #分别join四个线程到主线程 thread.join() results = [] #定义一个空的列表results，将四个线运行后保存在队列中的结果返回给空列表results for _ in range(4): results.append(q.get()) #q.get()按顺序从q中拿出一个值 print(results)if __name__ == '__main__': #if __name__ == '__main__'的意思是：当.py文件被直接运行时，if __name__ == '__main__'之下的代码块将被运行；当.py文件以模块形式被导入时，if __name__ == '__main__'之下的代码块不被运行。 multithreading() #多线程运算函数 GIL不一定有效率Python 的设计上, 有一个必要的环节, 就是 Global Interpreter Lock (GIL). 这个东西让 Python 还是一次性只能处理一个东西。这时候用多进程运算会卡。 线程锁Lock123456789101112131415161718192021222324252627import threading #导入线程标准模块def job1(): global A, lock #lock的函数 lock.acquire() #开始lock，job2不会接触job1的程序 for i in range(10): A += 1 print('job1', A) lock.release() #关闭lockdef job2(): global A, lock lock.acquire() for i in range(10): A += 10 print('job2', A) lock.release()if __name__ == '__main__': lock = threading.Lock() #一个大写一个小写 A = 0 t1 = threading.Thread(target=job1) t2 = threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() 多进程添加进程导入模块123456789101112import multiprocessing as mp #导入模块import threading as tddef job(a,d): print('aaaaa')t1 = td.Thread(target=job,args=(1,2)) #job没有括号，有括号就是调用，这儿是引用这个目标罢了p1 = mp.Process(target=job,args=(1,2))t1.start()p1.start()t1.join()p1.join() 完整代码：123456789import multiprocessing as mpdef job(a,d): print('aaaaa')if __name__=='__main__': #要在这个函数下面运行，不然报错，同时mac要在terminal下面 p1 = mp.Process(target=job,args=(1,2)) p1.start() p1.join() 把值储存在queue里面job本身没有返回值的原因才做这个12345678910111213141516171819import multiprocessing as mpdef job(q): res = 0 for i in range(1000): res += i+i**2+i**3 q.put(res) # 放进q里面if __name__ == '__main__': q = mp.Queue() # q中存放返回值，代替return的返回值 。这是第一步 p1 = mp.Process(target=job, args=(q,)) #定义两个线程函数，用来处理同一个任务, args 的参数只要一个值的时候，参数后面需要加一个逗号，表示args是可迭代的，后面可能还有别的参数，不加逗号会出错 p2 = mp.Process(target=job, args=(q,)) p1.start() p2.start() p1.join() p2.join() res1 = q.get() res2 = q.get() print(res1+res2) 哪个有效率123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import multiprocessing as mpimport threading as tdimport timedef job(q): res = 0 for i in range(1000000): res += i+i**2+i**3 q.put(res) # queuedef multicore(): q = mp.Queue() p1 = mp.Process(target=job, args=(q,)) p2 = mp.Process(target=job, args=(q,)) p1.start() p2.start() p1.join() p2.join() res1 = q.get() res2 = q.get() print('multicore:' , res1+res2)def normal(): res = 0 for _ in range(2): for i in range(1000000): res += i+i**2+i**3 print('normal:', res)def multithread(): q = mp.Queue() t1 = td.Thread(target=job, args=(q,)) t2 = td.Thread(target=job, args=(q,)) t1.start() t2.start() t1.join() t2.join() res1 = q.get() res2 = q.get() print('multithread:', res1+res2)if __name__ == '__main__': st = time.time() normal() st1= time.time() print('normal time:', st1 - st) multithread() st2 = time.time() print('multithread time:', st2 - st1) multicore() print('multicore time:', time.time()-st2) 进程池12345678910111213141516import multiprocessing as mpdef job(x): return x*x #pool里面有返回值 ，所以是returndef multicore(): pool = mp.Pool(processes=2) #定义进程池，大写P，这时候有返回值，这是第一步,默认值是所有的核，这边用2个核 res = pool.map(job, range(10)) print(res) # 0.1.4.9....81 res = pool.apply_async(job, (2,)) #apply_async，只给一个值，只在一个核里面运行一次。传入值时要注意是可迭代的，所以在传入值后需要加逗号, 同时需要用get()方法获取返回值 print(res.get()) #结果是4 multi_res =[pool.apply_async(job, (i,)) for i in range(10)] #这边是为了输出很多个值，用迭代器的功能 print([res.get() for res in multi_res]) #结果也在列表中迭代if __name__ == '__main__': multicore() map是Python的内置函数, 使用的方式如下; list = map(func, iter)其中， func是函数， iter是可迭代的序列。它的功能是：将一个序列中的每一个元素应用传入的函数， 并返回一个包含所有函数调用结果的一个列表. 共享内存共享内存 shared memory,不能多进程分享全局变量12345import multiprocessing as mpvalue1 = mp.Value('i', 0) # 形式 ， 内容value2 = mp.Value('d', 3.14)array = mp.Array('i', [1, 2, 3, 4]) # 整数，一维列表 进程锁Lock，还是要先定义共享内存1234567891011121314151617181920import multiprocessing as mpimport timedef job(v, num): for _ in range(5): time.sleep(0.1) # 暂停0.1秒，让输出效果更明显 v.value += num # v.value获取共享变量值 print(v.value, end=\"\") def multicore(): v = mp.Value('i', 0) # 定义共享变量 p1 = mp.Process(target=job, args=(v,1)) p2 = mp.Process(target=job, args=(v,3)) # 设定不同的number看如何抢夺内存 p1.start() p2.start() p1.join() p2.join() if __name__ == '__main__': multicore() 上门会抢夺进程资源，下面是lock1234567891011121314151617181920def job(v, num, l): # 需要将lock传入 第二步 l.acquire() # 锁住 第三步 for _ in range(5): time.sleep(0.1) v.value += num # 获取共享内存 print(v.value) l.release() # 释放def multicore(): l = mp.Lock() # 定义一个进程锁,第一步 v = mp.Value('i', 0) # 定义共享内存 p1 = mp.Process(target=job, args=(v,1,l)) # 需要将lock传入 p2 = mp.Process(target=job, args=(v,3,l)) #p2 基于p1已经完成的基础上再加3 p1.start() p2.start() p1.join() p2.join()if __name__ == '__main__': multicore() tkinker窗口一个GUI界面，实现一个直观的窗口 label&amp;button标签和按钮 导入tkinter模块 建立函数名1234567891011121314151617181920212223242526272829303132import tkinter as tkwindow = tk.Tk() #对象都是要大写的window.title('my window') #名字window.geometry('200x100') #大小# 这里是窗口的内容var = tk.StringVar() # 这时文字变量储存器l = tk.Label(window, textvariable=var, # 使用 textvariable 替换 text, 因为这个可以变化 bg='green', font=('Arial', 12), width=15, height=2)l.pack() # 固定窗口位置，place()某个点的值# 添加按钮b = tk.Button(window, text='hit me', # 显示在按钮上的文字，还是要大写B width=15, height=2, command=hit_me) # 点击按钮式执行的命令，就是hit_ne的函数b.pack() # 按钮位置# 定义hit_me函数on_hit = False # 默认初始状态为 Falsedef hit_me(): global on_hit if on_hit == False: # 从 False 状态变成 True 状态 on_hit = True var.set('you hit me') # 设置标签的文字为 'you hit me' else: # 从 True 状态变成 False 状态 on_hit = False var.set('') # 设置 文字为空window.mainloop() #最后一步，不断刷新 输入输出文本框 entry 和 text12345678910111213141516171819202122232425262728293031323334import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')#定义一个输入框# e = tk.Entry(window, show=\"*\")e = tk.Entry(window, show=\"1\") #创建输入框entry，用户输入任何内容都显示为*,对象还是要大写e.pack()#定义两个按钮的函数def insert_point(): var = e.get() #获得文本框的值用get() t.insert('insert', var) #指针所在就是'insert'的形式def insert_end(): var = e.get() # t.insert('end', var) 放在尾部就是'end'这个形式 t.insert(2.2, var) #具体地点，某行某列#定义两个按钮b1 = tk.Button(window, text='insert point', width=15, height=2, command=insert_point) #这边两个函数b1.pack()b2 = tk.Button(window, text='insert end', command=insert_end)b2.pack()#定义一个文本框t = tk.Text(window, height=2)t.pack()window.mainloop() Listbox，列表部件1234567891011121314151617181920212223242526272829303132333435import tkinter as tk#定义一个窗口视图大小window = tk.Tk()window.title('my window')window.geometry('200x200')#定义label现实，因为是变量var1 = tk.StringVar()l = tk.Label(window, bg='yellow', width=4, textvariable=var1)l.pack()# 定义光标选中的列表数字传入label的函数def print_selection(): value = lb.get(lb.curselection()) #得到列表选中的lb值 var1.set(value) #交给var1#定义buttonb1 = tk.Button(window, text='print selection', width=15, height=2, command=print_selection)b1.pack()#定义Listboxvar2 = tk.StringVar() #因为是变量var2.set((11,22,33,44)) # 定义数据lb = tk.Listbox(window, listvariable=var2) #定义列表框，列表传入参数list_items = [1,2,3,4] #列表的indexfor item in list_items: #迭代插入lb的数据到位置 lb.insert('end', item)lb.insert(1, 'first')lb.insert(2, 'second')lb.delete(2)lb.pack()window.mainloop() 选择按钮123456789101112131415161718192021222324252627import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')var = tk.StringVar()l = tk.Label(window, bg='yellow', width=20, text='empty')l.pack()def print_selection(): #print_selection 功能就是选择了某个 radiobutton 后我们会在屏幕上打印的选项. l.config(text='you have selected ' + var.get())#config()函数表示里面所有的参数都可以被改变，这个改变了上面text里面的emptyr1 = tk.Radiobutton(window, text='Option A', #装在window里面 variable=var, value='A',#当我们鼠标选中了其中一个选项，把value的值A放到变量var中，然后赋值给variable command=print_selection)r1.pack()r2 = tk.Radiobutton(window, text='Option B', variable=var, value='B', command=print_selection)r2.pack()r3 = tk.Radiobutton(window, text='Option C', variable=var, value='C', command=print_selection)r3.pack()window.mainloop() Scale尺寸可以拉动，返回的是一个数字1234567891011121314151617import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')l = tk.Label(window, bg='yellow', width=20, text='empty')l.pack()def print_selection(v): #这个有默认的返回值，所以加v l.config(text='you have selected ' + v)s = tk.Scale(window, label='try me', from_=5, to=11, orient=tk.HORIZONTAL, length=200, showvalue=0, tickinterval=2, resolution=0.01, command=print_selection)s.pack()window.mainloop() 参数from_=5，to=11的意思就是从5到11，即这个滚动条最小值为5，最大值为11（这里使用from_是因为在python中有from这个关键词）参数orient=tk.HORIZONTAL在这里就是设置滚动条的方向，如我们所看到的效果图，这里HORIZONTAL就是横向。要加tk参数length这里是指滚动条部件的长度，但注意的是和其他部件width表示不同，width表示的是以字符为单位，比如width=4，就是4个字符的长度，而此处的length=200，是指我们常用的像素为单位，即长度为200个像素参数resolution=0.01这里我们可以借助数学题来理解，我们做的很多数学题都会让我们来保留几位小数，此处的0.01就是保留2位小数，showvalue=0显示的就是效果图，上方无结果显示，如果改为showvalue=1参数tickinterval设置的就是坐标的间隔，此处为tickinterval=2， checkbutton 勾选和radiobutton类似重点就是参数onvalue和offvalue的值，传入variable里面，同时定义传入的var是IntVar()的函数123456789101112131415161718192021222324252627282930import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')l = tk.Label(window, bg='yellow', width=20, text='empty')l.pack()def print_selection(): if (var1.get() == 1) &amp; (var2.get() == 0): l.config(text='I love only Python ') elif (var1.get() == 0) &amp; (var2.get() == 1): l.config(text='I love only C++') elif (var1.get() == 0) &amp; (var2.get() == 0): l.config(text='I do not love either') else: l.config(text='I love both')var1 = tk.IntVar()var2 = tk.IntVar()c1 = tk.Checkbutton(window, text='Python', variable=var1, onvalue=1, offvalue=0, command=print_selection)c2 = tk.Checkbutton(window, text='C++', variable=var2, onvalue=1, offvalue=0, command=print_selection)c1.pack()c2.pack()window.mainloop() Canvas 画布12345678910111213141516171819202122import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')canvas = tk.Canvas(window, bg='blue', height=100, width=200) #加载画布 image_file = tk.PhotoImage(file='ins.gif') #载入图片image = canvas.create_image(10, 10, anchor='nw', image=image_file) #左上角是铆点x0, y0, x1, y1= 50, 50, 80, 80line = canvas.create_line(x0, y0, x1, y1)oval = canvas.create_oval(x0, y0, x1, y1, fill='red') #创建一个圆，填充色为`red`红色arc = canvas.create_arc(x0+30, y0+30, x1+30, y1+30, start=0, extent=180) #创建一个扇形rect = canvas.create_rectangle(100, 30, 100+20, 30+20) #创建一个矩形canvas.pack()def moveit(): #canvas.move(rect, 0, 2)的参数(rect,0,2)就是移动rect这个变量，即我们看到的矩形 后面的0和2，也就是横坐标移动0个单位 canvas.move(rect, 0, 2)b = tk.Button(window, text='move', command=moveit).pack()window.mainloop() 菜单栏 menubar12345678910111213141516171819202122232425262728293031323334353637383940import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')l = tk.Label(window, text='', bg='yellow')l.pack()counter = 0def do_job(): global counter l.config(text='do '+ str(counter)) counter+=1##创建一个菜单栏，这里我们可以把他理解成一个容器，在窗口的上方menubar = tk.Menu(window)filemenu = tk.Menu(menubar, tearoff=0) ##定义一个空菜单单元menubar.add_cascade(label='File', menu=filemenu) ##将上面定义的空菜单命名为`File`，放在菜单栏中，就是装入那个容器中##在`File`中加入`New`的小菜单，即我们平时看到的下拉菜单，每一个小菜单对应命令操作。##如果点击这些单元, 就会触发`do_job`的功能filemenu.add_command(label='New', command=do_job)filemenu.add_command(label='Open', command=do_job)filemenu.add_command(label='Save', command=do_job)##同样的在`File`中加入`Open`小菜单filemenu.add_separator() ##这里就是一条分割线filemenu.add_command(label='Exit', command=window.quit) ##同样的在`File`中加入`Exit`小菜单,此处对应命令为`window.quit`editmenu = tk.Menu(menubar, tearoff=0)menubar.add_cascade(label='Edit', menu=editmenu)editmenu.add_command(label='Cut', command=do_job)editmenu.add_command(label='Copy', command=do_job)editmenu.add_command(label='Paste', command=do_job)submenu = tk.Menu(filemenu) ##和上面定义菜单一样，不过此处实在`File`上创建一个空的菜单filemenu.add_cascade(label='Import', menu=submenu, underline=0) ##给放入的菜单`submenu`命名为`Import`submenu.add_command(label=\"Submenu1\", command=do_job) ##这里和上面也一样，在`Import`中加入一个小菜单命令`Submenu1`window.config(menu=menubar) #给window改menu属性，这个是特别的地方window.mainloop() Frame 框架12345678910111213141516171819202122import tkinter as tkwindow = tk.Tk()window.title('my window')window.geometry('200x200')tk.Label(window, text='on the window').pack() ###定义一个`label`显示`on the window`###在`window`上创建一个`frame`frm = tk.Frame(window)frm.pack()###在刚刚创建的`frame`上创建两个`frame`，我们可以把它理解成一个大容器里套了一个小容器，即`frm`上有两个`frame` ，`frm_l`和`frm_r`frm_l = tk.Frame(frm, )frm_r = tk.Frame(frm)###这里是控制小的`frm`部件在大的`frm`的相对位置，此处`frm_l`就是在`frm`的左边，`frm_r`在`frm`的右边frm_l.pack(side='left')frm_r.pack(side='right')###这里的三个label就是在我们创建的frame上定义的label部件，还是以容器理解，就是容器上贴了标签，来指明这个是什么，解释这个容器。tk.Label(frm_l, text='on the frm_l1').pack()tk.Label(frm_l, text='on the frm_l2').pack()tk.Label(frm_r, text='on the frm_r1').pack()window.mainloop() Message弹窗tk.messagebox.showinfo(title=’’,message=’’)#提示信息对话窗tk.messagebox.showwarning()#提出警告对话窗tk.messagebox.showerror()#提出错误对话窗tk.messagebox.askquestion()#询问选择对话窗 注意是小写12345678910111213141516171819import tkinter as tkimport tkinter.messageboxwindow = tk.Tk()window.title('my window')window.geometry('200x200')def hit_me(): #tk.messagebox.showinfo(title='Hi', message='hahahaha') # return 'ok' #tk.messagebox.showwarning(title='Hi', message='nononono') # return 'ok' #tk.messagebox.showerror(title='Hi', message='No!! never') # return 'ok' #print(tk.messagebox.askquestion(title='Hi', message='hahahaha')) # return 'yes' , 'no' 有返回值 #print(tk.messagebox.askyesno(title='Hi', message='hahahaha')) # return True, False print(tk.messagebox.asktrycancel(title='Hi', message='hahahaha')) # return True, False print(tk.messagebox.askokcancel(title='Hi', message='hahahaha')) # return True, False print(tk.messagebox.askyesnocancel(title=\"Hi\", message=\"haha\")) # return, True, False, Nonetk.Button(window, text='hit me', command=hit_me).pack()window.mainloop() pack，grid，place放置位置pack(),参数是side1234tk.Label(window, text='1').pack(side='top')#上tk.Label(window, text='1').pack(side='bottom')#下tk.Label(window, text='1').pack(side='left')#左tk.Label(window, text='1').pack(side='right')#右 grid接下里我们在看看grid(), grid 是方格, 所以所有的内容会被放在这些规律的方格中.以上的代码就是创建一个四行三列的表格，其实grid就是用表格的形式定位的。这里的参数 row为行，colum为列，padx就是单元格左右间距，pady就是单元格上下间距。123for i in range(4): #行 for j in range(3): #列 tk.Label(window, text=1).grid(row=i, column=j, padx=10, pady=10) #内部拓展，ipadx place再接下来就是place(), 这个比较容易理解，就是给精确的坐标来定位，如此处给的（20,10），就是将这个部件放在坐标为（x，y）的这个位置 后面的参数anchor=nw就是前面所讲的锚定点是西北角。1tk.Label(window, text=1).place(x=20, y=10, anchor='nw') 登录窗口https://morvanzhou.github.io/tutorials/python-basic/tkinter/3-03-example3/12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import tkinter as tkfrom tkinter import messagebox # import this to fix messagebox errorimport picklewindow = tk.Tk()window.title('Welcome to Mofan Python')window.geometry('450x300')# welcome imagecanvas = tk.Canvas(window, height=200, width=500)image_file = tk.PhotoImage(file='welcome.gif')image = canvas.create_image(0,0, anchor='nw', image=image_file)canvas.pack(side='top')# user informationtk.Label(window, text='User name: ').place(x=50, y= 150)tk.Label(window, text='Password: ').place(x=50, y= 190)var_usr_name = tk.StringVar()var_usr_name.set('example@python.com')entry_usr_name = tk.Entry(window, textvariable=var_usr_name)entry_usr_name.place(x=160, y=150)var_usr_pwd = tk.StringVar()entry_usr_pwd = tk.Entry(window, textvariable=var_usr_pwd, show='*')entry_usr_pwd.place(x=160, y=190)def usr_login(): usr_name = var_usr_name.get() usr_pwd = var_usr_pwd.get() try: with open('usrs_info.pickle', 'rb') as usr_file: usrs_info = pickle.load(usr_file) except FileNotFoundError: with open('usrs_info.pickle', 'wb') as usr_file: usrs_info = {'admin': 'admin'} pickle.dump(usrs_info, usr_file) if usr_name in usrs_info: if usr_pwd == usrs_info[usr_name]: tk.messagebox.showinfo(title='Welcome', message='How are you? ' + usr_name) else: tk.messagebox.showerror(message='Error, your password is wrong, try again.') else: is_sign_up = tk.messagebox.askyesno('Welcome', 'You have not signed up yet. Sign up today?') if is_sign_up: usr_sign_up()def usr_sign_up(): def sign_to_Mofan_Python(): np = new_pwd.get() npf = new_pwd_confirm.get() nn = new_name.get() with open('usrs_info.pickle', 'rb') as usr_file: exist_usr_info = pickle.load(usr_file) if np != npf: tk.messagebox.showerror('Error', 'Password and confirm password must be the same!') elif nn in exist_usr_info: tk.messagebox.showerror('Error', 'The user has already signed up!') else: exist_usr_info[nn] = np with open('usrs_info.pickle', 'wb') as usr_file: pickle.dump(exist_usr_info, usr_file) tk.messagebox.showinfo('Welcome', 'You have successfully signed up!') window_sign_up.destroy() window_sign_up = tk.Toplevel(window) window_sign_up.geometry('350x200') window_sign_up.title('Sign up window') new_name = tk.StringVar() new_name.set('example@python.com') tk.Label(window_sign_up, text='User name: ').place(x=10, y= 10) entry_new_name = tk.Entry(window_sign_up, textvariable=new_name) entry_new_name.place(x=150, y=10) new_pwd = tk.StringVar() tk.Label(window_sign_up, text='Password: ').place(x=10, y=50) entry_usr_pwd = tk.Entry(window_sign_up, textvariable=new_pwd, show='*') entry_usr_pwd.place(x=150, y=50) new_pwd_confirm = tk.StringVar() tk.Label(window_sign_up, text='Confirm password: ').place(x=10, y= 90) entry_usr_pwd_confirm = tk.Entry(window_sign_up, textvariable=new_pwd_confirm, show='*') entry_usr_pwd_confirm.place(x=150, y=90) btn_comfirm_sign_up = tk.Button(window_sign_up, text='Sign up', command=sign_to_Mofan_Python) btn_comfirm_sign_up.place(x=150, y=130)# login and sign up buttonbtn_login = tk.Button(window, text='Login', command=usr_login)btn_login.place(x=170, y=230)btn_sign_up = tk.Button(window, text='Sign up', command=usr_sign_up)btn_sign_up.place(x=270, y=230)window.mainloop() pickle模块pickle 是一个 python 中, 压缩/保存/提取 文件的模块. 最一般的使用方式非常简单. 比如下面就是压缩并保存一个字典的方式. 字典和列表都是能被保存的.1234567891011121314import picklea_dict = {'da': 111, 2: [23,1,4], '23': {1:2,'d':'sad'}}# pickle a variable to a filefile = open('pickle_example.pickle', 'wb') #wb 是以写的形式打开pickle.dump(a_dict, file) #装载的是a_dict,装在flie里面(固定在文件列表里面)，就有一个pickle_example.pickle file.close()# reload a file to a variablewith open('pickle_example.pickle', 'rb') as file: #rb 是读的形式打开，with不用考虑关闭file.close() a_dict1 =pickle.load(file)print(a_dict1)","link":"/2017/09/23/python-文档/"}],"tags":[{"name":"wordpress","slug":"wordpress","link":"/tags/wordpress/"},{"name":"碎碎念","slug":"碎碎念","link":"/tags/碎碎念/"},{"name":"安全","slug":"安全","link":"/tags/安全/"},{"name":"LAMP","slug":"LAMP","link":"/tags/LAMP/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"ml","slug":"ml","link":"/tags/ml/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/技术/"},{"name":"生活","slug":"生活","link":"/categories/生活/"}]}