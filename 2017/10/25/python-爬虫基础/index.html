<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>python 爬虫基础 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="做了一个整理，把python的爬虫基础发一下。这个是基于周莫凡的python整理材料 [TOC]">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="python 爬虫基础">
<meta property="og:url" content="http://yoursite.com/2017/10/25/python-爬虫基础/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="做了一个整理，把python的爬虫基础发一下。这个是基于周莫凡的python整理材料 [TOC]">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-11-25T14:24:47.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="python 爬虫基础">
<meta name="twitter:description" content="做了一个整理，把python的爬虫基础发一下。这个是基于周莫凡的python整理材料 [TOC]">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-python-爬虫基础" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/10/25/python-爬虫基础/" class="article-date">
  <time datetime="2017-10-25T14:03:05.000Z" itemprop="datePublished">2017-10-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/技术/">技术</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      python 爬虫基础
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>做了一个整理，把python的爬虫基础发一下。这个是基于周莫凡的python整理材料</p>
<p>[TOC]<br><a id="more"></a></p>
<h1 id="网页爬虫"><a href="#网页爬虫" class="headerlink" title="网页爬虫"></a>网页爬虫</h1><h2 id="了解网页结构"><a href="#了解网页结构" class="headerlink" title="了解网页结构"></a>了解网页结构</h2><p>用python登录网页：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="comment"># if has Chinese, apply decode()</span></span><br><span class="line">html = urlopen(</span><br><span class="line">    <span class="string">"https://morvanzhou.github.io/static/scraping/basic-structure.html"</span></span><br><span class="line">).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(html)</span><br></pre></td></tr></table></figure></p>
<p>然后用正则表达式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res = re.findall(<span class="string">r"&lt;p&gt;(.*?)&lt;/p&gt;"</span>, html, flags=re.DOTALL)    <span class="comment"># re.DOTALL if multi line</span></span><br><span class="line">print(<span class="string">"\nPage paragraph is: "</span>, res[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="BeautifulSoup-解析网页"><a href="#BeautifulSoup-解析网页" class="headerlink" title="BeautifulSoup 解析网页"></a>BeautifulSoup 解析网页</h2><h3 id="BS基础"><a href="#BS基础" class="headerlink" title="BS基础"></a>BS基础</h3><p>选着要爬的网址 (url)<br>使用 python 登录上这个网址 (urlopen等)<br>读取网页信息 (read() 出来)<br>将读取的信息放入 BeautifulSoup<br>使用 BeautifulSoup 选取 tag 信息等 (代替正则表达式)<br>安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Python 3+</span><br><span class="line">pip3 install beautifulsoup4</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="comment"># if has Chinese, apply decode()</span></span><br><span class="line">html = urlopen(<span class="string">"https://morvanzhou.github.io/static/scraping/basic-structure.html"</span>).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(html)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这边的解析方式有很多 https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ 推荐使用lxml</span></span><br><span class="line">soup = BeautifulSoup(html, features=<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.h1) <span class="comment">#直接h1</span></span><br><span class="line">print(<span class="string">'\n'</span>, soup.p)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">all_href = soup.find_all(<span class="string">'a'</span>) <span class="comment">#</span></span><br><span class="line">all_href = [l[<span class="string">'href'</span>] <span class="keyword">for</span> l <span class="keyword">in</span> all_href]</span><br><span class="line">print(<span class="string">'\n'</span>, all_href)</span><br></pre></td></tr></table></figure>
<h3 id="BS-解析CSS"><a href="#BS-解析CSS" class="headerlink" title="BS 解析CSS"></a>BS 解析CSS</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#还是导入两个模块</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="comment"># if has Chinese, apply decode()</span></span><br><span class="line">html = urlopen(<span class="string">"https://morvanzhou.github.io/static/scraping/list.html"</span>).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(html)</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html, features=<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use class to narrow search</span></span><br><span class="line">month = soup.find_all(<span class="string">'li'</span>, &#123;<span class="string">"class"</span>: <span class="string">"month"</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> month:</span><br><span class="line">    print(m.get_text()) <span class="comment">#这边用get_text()来获得里面的具体信息</span></span><br><span class="line"></span><br><span class="line">jan = soup.find(<span class="string">'ul'</span>, &#123;<span class="string">"class"</span>: <span class="string">'jan'</span>&#125;)</span><br><span class="line">d_jan = jan.find_all(<span class="string">'li'</span>)              <span class="comment"># use jan as a parent</span></span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> d_jan:</span><br><span class="line">    print(d.get_text())</span><br></pre></td></tr></table></figure>
<h3 id="BS与正则表达式的运用"><a href="#BS与正则表达式的运用" class="headerlink" title="BS与正则表达式的运用"></a>BS与正则表达式的运用</h3><ol>
<li>导入正则模块</li>
<li>re.compile()</li>
<li>迭代输出,输出的是列表<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># if has Chinese, apply decode()</span></span><br><span class="line">html = urlopen(<span class="string">"https://morvanzhou.github.io/static/scraping/table.html"</span>).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">print(html)</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html, features=<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">img_links = soup.find_all(<span class="string">"img"</span>, &#123;<span class="string">"src"</span>: re.compile(<span class="string">'.*?\.jpg'</span>)&#125;)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> img_links:</span><br><span class="line">    print(link[<span class="string">'src'</span>])</span><br><span class="line">    </span><br><span class="line">course_links = soup.find_all(<span class="string">'a'</span>, &#123;<span class="string">'href'</span>: re.compile(<span class="string">'https://morvan.*'</span>)&#125;)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> course_links:</span><br><span class="line">    print(link[<span class="string">'href'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="爬百度百科-重要"><a href="#爬百度百科-重要" class="headerlink" title="爬百度百科 重要"></a>爬百度百科 重要</h3><p><strong><a href="https://morvanzhou.github.io/tutorials/data-manipulation/scraping/2-04-practice-baidu-baike/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/data-manipulation/scraping/2-04-practice-baidu-baike/</a></strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#Practice: scrape Baidu Baike</span></span><br><span class="line"><span class="comment">#Here we build a scraper to crawl Baidu Baike from this page onwards. We store a historical webpage that we have already visited to keep tracking it.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#In [2]:</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">base_url = <span class="string">"https://baike.baidu.com"</span></span><br><span class="line">his = [<span class="string">"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Select the last sub url in "his", print the title and url.</span></span><br><span class="line"></span><br><span class="line">url = base_url + his[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">html = urlopen(url).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">soup = BeautifulSoup(html, features=<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find(<span class="string">'h1'</span>).get_text(), <span class="string">'    url: '</span>, his[<span class="number">-1</span>])</span><br><span class="line"><span class="comment">#网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711</span></span><br><span class="line"><span class="comment">#Find all sub_urls for baidu baike (item page), randomly select a sub_urls and store it in "his". If no valid sub link is found, than pop last url in "his".</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># find valid urls</span></span><br><span class="line">sub_urls = soup.find_all(<span class="string">"a"</span>, &#123;<span class="string">"target"</span>: <span class="string">"_blank"</span>, <span class="string">"href"</span>: re.compile(<span class="string">"/item/(%.&#123;2&#125;)+$"</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(sub_urls) != <span class="number">0</span>:</span><br><span class="line">    his.append(random.sample(sub_urls, <span class="number">1</span>)[<span class="number">0</span>][<span class="string">'href'</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># no valid sub link found</span></span><br><span class="line">    his.pop()</span><br><span class="line">print(his)</span><br><span class="line"><span class="comment">#['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E4%B8%8B%E8%BD%BD%E8%80%85']</span></span><br><span class="line"><span class="comment">#Put everthing together. Random running for 20 iterations. See what we end up with.</span></span><br><span class="line"><span class="comment">#his = ["/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711"]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    url = base_url + his[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    html = urlopen(url).read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    soup = BeautifulSoup(html, features=<span class="string">'lxml'</span>)</span><br><span class="line">    print(i, soup.find(<span class="string">'h1'</span>).get_text(), <span class="string">'    url: '</span>, his[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># find valid urls</span></span><br><span class="line">    sub_urls = soup.find_all(<span class="string">"a"</span>, &#123;<span class="string">"target"</span>: <span class="string">"_blank"</span>, <span class="string">"href"</span>: re.compile(<span class="string">"/item/(%.&#123;2&#125;)+$"</span>)&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(sub_urls) != <span class="number">0</span>:</span><br><span class="line">        his.append(random.sample(sub_urls, <span class="number">1</span>)[<span class="number">0</span>][<span class="string">'href'</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># no valid sub link found</span></span><br><span class="line">        his.pop()</span><br></pre></td></tr></table></figure></p>
<h2 id="更多请求-下载方式"><a href="#更多请求-下载方式" class="headerlink" title="更多请求/下载方式"></a>更多请求/下载方式</h2><h3 id="Request的方式"><a href="#Request的方式" class="headerlink" title="Request的方式"></a>Request的方式</h3><p>Requests：get/post<br>我们就来说两个重要的, get, post, 95% 的时间, 你都是在使用这两个来请求一个网页.<br>post：<br> 账号登录<br> 搜索内容<br> 上传图片<br> 上传文件<br> 往服务器传数据 等<br>get：<br> 正常打开网页<br> 不往服务器传数据</p>
<p>网页使用 get 就可以了, 都是只是 get 发送请求. 而 post, 我们则是给服务器发送个性化请求, 比如将你的账号密码传给服务器, 让它给你返回一个含有你个人信息的 HTML.</p>
<p>从主动和被动的角度来说, post 中文是发送, 比较主动, 你控制了服务器返回的内容. 而 get 中文是取得, 是被动的, 你没有发送给服务器个性化的信息, 它不会根据你个性化的信息返回不一样的 HTML</p>
<p>安装Requests<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python 3+</span></span><br><span class="line">pip3 install requests</span><br></pre></td></tr></table></figure></p>
<p>get有一个参数 params，就是Request.post<br>用Session来传递cookie,就是Session.get<br>注意post的地址，get地址里面没有现实的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#requests: an alternative to urllib</span></span><br><span class="line"><span class="comment">#requests has more functions to replace urlopen. Use request.get() to replace urlopen() and pass some parameters to the webpage. The webbrowser is to open the new url and give you an visualization of this result.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> webbrowser <span class="comment">#内置模块，打开浏览器</span></span><br><span class="line">param = &#123;<span class="string">"wd"</span>: <span class="string">"莫烦Python"</span>&#125;</span><br><span class="line">r = requests.get(<span class="string">'http://www.baidu.com/s'</span>, params=param)</span><br><span class="line">print(r.url)</span><br><span class="line">webbrowser.open(r.url)</span><br><span class="line">http://www.baidu.com/s?wd=%E8%8E%AB%E7%83%A6Python</span><br><span class="line"><span class="comment">#Out[2]:</span></span><br><span class="line"><span class="comment">#True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##post</span></span><br><span class="line"><span class="comment">##We test the post function in this page. To pass some data to the server to analyse and send some response to you accordingly.</span></span><br><span class="line">data = &#123;<span class="string">'firstname'</span>: <span class="string">'莫烦'</span>, <span class="string">'lastname'</span>: <span class="string">'周'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://pythonscraping.com/files/processing.php'</span>, data=data)</span><br><span class="line">print(r.text)</span><br><span class="line"><span class="comment">#Hello there, 莫烦 周!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##upload image</span></span><br><span class="line"><span class="comment">##We still use post function to update image in this page.</span></span><br><span class="line">file = &#123;<span class="string">'uploadFile'</span>: open(<span class="string">'./image.png'</span>, <span class="string">'rb'</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://pythonscraping.com/files/processing2.php'</span>, files=file)</span><br><span class="line">print(r.text)</span><br><span class="line"><span class="comment">#The file image.png has been uploaded.</span></span><br><span class="line"><span class="comment">#login</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##Use post method to login to a website.</span></span><br><span class="line">payload = &#123;<span class="string">'username'</span>: <span class="string">'Morvan'</span>, <span class="string">'password'</span>: <span class="string">'password'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://pythonscraping.com/pages/cookies/welcome.php'</span>, data=payload)</span><br><span class="line">print(r.cookies.get_dict())</span><br><span class="line">r = requests.get(<span class="string">'http://pythonscraping.com/pages/cookies/profile.php'</span>, cookies=r.cookies)</span><br><span class="line">print(r.text)</span><br><span class="line"><span class="comment">#&#123;'username': 'Morvan', 'loggedin': '1'&#125;</span></span><br><span class="line"><span class="comment">#Hey Morvan! Looks like you're still logged into the site!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##another general way to login</span></span><br><span class="line"><span class="comment">##Use session instead requests. Keep you in a session and keep track the cookies.</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">payload = &#123;<span class="string">'username'</span>: <span class="string">'Morvan'</span>, <span class="string">'password'</span>: <span class="string">'password'</span>&#125;</span><br><span class="line">r = session.post(<span class="string">'http://pythonscraping.com/pages/cookies/welcome.php'</span>, data=payload)</span><br><span class="line">print(r.cookies.get_dict())</span><br><span class="line">r = session.get(<span class="string">"http://pythonscraping.com/pages/cookies/profile.php"</span>)</span><br><span class="line">print(r.text)</span><br><span class="line"><span class="comment">#&#123;'username': 'Morvan', 'loggedin': '1'&#125;</span></span><br><span class="line"><span class="comment">#Hey Morvan! Looks like you're still logged into the site!</span></span><br></pre></td></tr></table></figure>
<h3 id="下载文件"><a href="#下载文件" class="headerlink" title="下载文件"></a>下载文件</h3><p>3种方式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#了下载到一个特定的文件夹, 我们先建立一个文件夹吧. 并且规定这个图片下载地址</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(<span class="string">'./img/'</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">IMAGE_URL = <span class="string">"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#方法1 使用 urlretrieve </span></span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve</span><br><span class="line">urlretrieve(IMAGE_URL, <span class="string">'./img/image1.png'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法2 使用 request </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(IMAGE_URL)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./img/image2.png'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#方法3 使用流的方式，不然只能先存在内存里</span></span><br><span class="line">r = requests.get(IMAGE_URL, stream=<span class="keyword">True</span>)    <span class="comment"># stream loading</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./img/image3.png'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> r.iter_content(chunk_size=<span class="number">32</span>):</span><br><span class="line">        f.write(chunk)</span><br></pre></td></tr></table></figure></p>
<h3 id="循环下载图片"><a href="#循环下载图片" class="headerlink" title="循环下载图片"></a>循环下载图片</h3><p>requests 访问和 下载功能, 还有 BeautifulSoup、<br>找到img_list下面。。。的img的src</p>
<ol>
<li>找到图片位置，分析<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">URL = <span class="string">"http://www.nationalgeographic.com.cn/animals/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># find list of image holder</span></span><br><span class="line">html = requests.get(URL).text</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">img_ul = soup.find_all(<span class="string">'ul'</span>, &#123;<span class="string">"class"</span>: <span class="string">"img_list"</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Create a folder for these pictures</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(<span class="string">'./img/'</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#download</span></span><br><span class="line"><span class="comment">#Find all picture urls and download them.In [4]:</span></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> img_ul:</span><br><span class="line">    imgs = ul.find_all(<span class="string">'img'</span>)<span class="comment"># find_all()函数</span></span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> imgs:</span><br><span class="line">        url = img[<span class="string">'src'</span>]</span><br><span class="line">        r = requests.get(url, stream=<span class="keyword">True</span>) <span class="comment">#用流的方式</span></span><br><span class="line">        image_name = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'./img/%s'</span> % image_name, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> chunk <span class="keyword">in</span> r.iter_content(chunk_size=<span class="number">128</span>):</span><br><span class="line">                f.write(chunk)</span><br><span class="line">        print(<span class="string">'Saved %s'</span> % image_name)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="加速爬虫"><a href="#加速爬虫" class="headerlink" title="加速爬虫"></a>加速爬虫</h2><h3 id="多进程分布式"><a href="#多进程分布式" class="headerlink" title="多进程分布式"></a>多进程分布式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 倒入模块</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen, urljoin</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">base_url = <span class="string">"http://127.0.0.1:4000/"</span></span><br><span class="line"><span class="comment"># base_url = 'https://morvanzhou.github.io/'</span></span><br><span class="line"><span class="comment"># DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAIN</span></span><br><span class="line"><span class="keyword">if</span> base_url != <span class="string">"http://127.0.0.1:4000/"</span>:</span><br><span class="line">    restricted_crawl = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    restricted_crawl = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Create a crawl function to open a url in parallel.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url)</span>:</span></span><br><span class="line">    response = urlopen(url) <span class="comment">#urlopen函数功能</span></span><br><span class="line">    time.sleep(<span class="number">0.1</span>)             <span class="comment"># slightly delay for downloading</span></span><br><span class="line">    <span class="keyword">return</span> response.read().decode()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Create a parse function to find all results we need in parallel</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    urls = soup.find_all(<span class="string">'a'</span>, &#123;<span class="string">"href"</span>: re.compile(<span class="string">'^/.+?/$'</span>)&#125;)</span><br><span class="line">    title = soup.find(<span class="string">'h1'</span>).get_text().strip()</span><br><span class="line">    page_urls = set([urljoin(base_url, url[<span class="string">'href'</span>]) <span class="keyword">for</span> url <span class="keyword">in</span> urls])</span><br><span class="line">    url = soup.find(<span class="string">'meta'</span>, &#123;<span class="string">'property'</span>: <span class="string">"og:url"</span>&#125;)[<span class="string">'content'</span>]</span><br><span class="line">    <span class="keyword">return</span> title, page_urls, url</span><br><span class="line"></span><br><span class="line"><span class="comment">#Normal way</span></span><br><span class="line"><span class="comment">#Do not use multiprocessing, test the speed. Firstly, set what urls we have already seen and what we haven't in a python set.</span></span><br><span class="line">unseen = set([base_url,])</span><br><span class="line">seen = set()</span><br><span class="line">count, t1 = <span class="number">1</span>, time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> len(unseen) != <span class="number">0</span>:       <span class="comment"># still get some url to visit</span></span><br><span class="line">    <span class="keyword">if</span> restricted_crawl <span class="keyword">and</span> len(seen) &gt; <span class="number">20</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">    print(<span class="string">'\nDistributed Crawling...'</span>)</span><br><span class="line">    htmls = [crawl(url) <span class="keyword">for</span> url <span class="keyword">in</span> unseen]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nDistributed Parsing...'</span>)</span><br><span class="line">    results = [parse(html) <span class="keyword">for</span> html <span class="keyword">in</span> htmls]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nAnalysing...'</span>)</span><br><span class="line">    seen.update(unseen)         <span class="comment"># seen the crawled</span></span><br><span class="line">    unseen.clear()              <span class="comment"># nothing unseen</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> title, page_urls, url <span class="keyword">in</span> results:</span><br><span class="line">        print(count, title, url)</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        unseen.update(page_urls - seen)     <span class="comment"># get new url to crawl</span></span><br><span class="line">print(<span class="string">'Total time: %.1f s'</span> % (time.time()-t1, ))    <span class="comment"># 53 s</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#multiprocessing</span></span><br><span class="line"><span class="comment">#Create a process pool and scrape parallelly.</span></span><br><span class="line">unseen = set([base_url,])</span><br><span class="line">seen = set()</span><br><span class="line"></span><br><span class="line">pool = mp.Pool(<span class="number">4</span>)                       </span><br><span class="line">count, t1 = <span class="number">1</span>, time.time()</span><br><span class="line"><span class="keyword">while</span> len(unseen) != <span class="number">0</span>:                 <span class="comment"># still get some url to visit</span></span><br><span class="line">    <span class="keyword">if</span> restricted_crawl <span class="keyword">and</span> len(seen) &gt; <span class="number">20</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">'\nDistributed Crawling...'</span>)</span><br><span class="line">    crawl_jobs = [pool.apply_async(crawl, args=(url,)) <span class="keyword">for</span> url <span class="keyword">in</span> unseen]</span><br><span class="line">    htmls = [j.get() <span class="keyword">for</span> j <span class="keyword">in</span> crawl_jobs]                                       <span class="comment"># request connection</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nDistributed Parsing...'</span>)</span><br><span class="line">    parse_jobs = [pool.apply_async(parse, args=(html,)) <span class="keyword">for</span> html <span class="keyword">in</span> htmls]</span><br><span class="line">    results = [j.get() <span class="keyword">for</span> j <span class="keyword">in</span> parse_jobs]                                     <span class="comment"># parse html</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\nAnalysing...'</span>)</span><br><span class="line">    seen.update(unseen)         <span class="comment"># seen the crawled</span></span><br><span class="line">    unseen.clear()              <span class="comment"># nothing unseen</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> title, page_urls, url <span class="keyword">in</span> results:</span><br><span class="line">        print(count, title, url)</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        unseen.update(page_urls - seen)     <span class="comment"># get new url to crawl</span></span><br><span class="line">print(<span class="string">'Total time: %.1f s'</span> % (time.time()-t1, ))    <span class="comment"># 16 s !!!</span></span><br></pre></td></tr></table></figure>
<h3 id="异步加载Asyncio"><a href="#异步加载Asyncio" class="headerlink" title="异步加载Asyncio"></a>异步加载Asyncio</h3><p>本质是单线程，GIL锁优化，3.5是原生库<br>爬网页用aiohttp<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"></span><br><span class="line"><span class="comment"># base_url = "https://morvanzhou.github.io/"</span></span><br><span class="line">base_url = <span class="string">"http://127.0.0.1:4000/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAIN</span></span><br><span class="line"><span class="keyword">if</span> base_url != <span class="string">"http://127.0.0.1:4000/"</span>:</span><br><span class="line">    restricted_crawl = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    restricted_crawl = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">seen = set()</span><br><span class="line">unseen = set([base_url])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    urls = soup.find_all(<span class="string">'a'</span>, &#123;<span class="string">"href"</span>: re.compile(<span class="string">'^/.+?/$'</span>)&#125;)</span><br><span class="line">    title = soup.find(<span class="string">'h1'</span>).get_text().strip()</span><br><span class="line">    page_urls = set([urljoin(base_url, url[<span class="string">'href'</span>]) <span class="keyword">for</span> url <span class="keyword">in</span> urls])</span><br><span class="line">    url = soup.find(<span class="string">'meta'</span>, &#123;<span class="string">'property'</span>: <span class="string">"og:url"</span>&#125;)[<span class="string">'content'</span>]</span><br><span class="line">    <span class="keyword">return</span> title, page_urls, url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url, session)</span>:</span></span><br><span class="line">    r = <span class="keyword">await</span> session.get(url)</span><br><span class="line">    html = <span class="keyword">await</span> r.text()</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">0.1</span>)        <span class="comment"># slightly delay for downloading</span></span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(loop)</span>:</span></span><br><span class="line">    pool = mp.Pool(<span class="number">8</span>)               <span class="comment"># slightly affected</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        count = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> len(unseen) != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'\nAsync Crawling...'</span>)</span><br><span class="line">            tasks = [loop.create_task(crawl(url, session)) <span class="keyword">for</span> url <span class="keyword">in</span> unseen]</span><br><span class="line">            finished, unfinished = <span class="keyword">await</span> asyncio.wait(tasks)</span><br><span class="line">            htmls = [f.result() <span class="keyword">for</span> f <span class="keyword">in</span> finished]</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'\nDistributed Parsing...'</span>)</span><br><span class="line">            parse_jobs = [pool.apply_async(parse, args=(html,)) <span class="keyword">for</span> html <span class="keyword">in</span> htmls]</span><br><span class="line">            results = [j.get() <span class="keyword">for</span> j <span class="keyword">in</span> parse_jobs]</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'\nAnalysing...'</span>)</span><br><span class="line">            seen.update(unseen)</span><br><span class="line">            unseen.clear()</span><br><span class="line">            <span class="keyword">for</span> title, page_urls, url <span class="keyword">in</span> results:</span><br><span class="line">                <span class="comment"># print(count, title, url)</span></span><br><span class="line">                unseen.update(page_urls - seen)</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(main(loop))</span><br><span class="line">    <span class="comment"># loop.close()</span></span><br><span class="line">    print(<span class="string">"Async total time: "</span>, time.time() - t1)</span><br></pre></td></tr></table></figure></p>
<h2 id="高级爬虫"><a href="#高级爬虫" class="headerlink" title="高级爬虫"></a>高级爬虫</h2><h3 id="Selenium控制浏览器"><a href="#Selenium控制浏览器" class="headerlink" title="Selenium控制浏览器"></a>Selenium控制浏览器</h3><p>安装：<br>pip3 install selenium<br>火狐有插件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(<span class="string">'./img/'</span>, exist_ok=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">"https://morvanzhou.github.io/"</span>)</span><br><span class="line">driver.find_element_by_xpath(<span class="string">u"//img[@alt='强化学习 (Reinforcement Learning)']"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">"About"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"赞助"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"教程 ▾"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"数据处理 ▾"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"网页爬虫"</span>).click()</span><br><span class="line">html = driver.page_source       <span class="comment"># get html</span></span><br><span class="line">driver.get_screenshot_as_file(<span class="string">"./img/sreenshot1.png"</span>)</span><br><span class="line">driver.close()</span><br><span class="line">print(html[:<span class="number">200</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果不要浏览器show</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">"--headless"</span>)       <span class="comment"># define headless</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add the option when creating driver</span></span><br><span class="line">driver = webdriver.Chrome(chrome_options=chrome_options)    </span><br><span class="line">driver.get(<span class="string">"https://morvanzhou.github.io/"</span>)</span><br><span class="line">driver.find_element_by_xpath(<span class="string">u"//img[@alt='强化学习 (Reinforcement Learning)']"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">"About"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"赞助"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"教程 ▾"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"数据处理 ▾"</span>).click()</span><br><span class="line">driver.find_element_by_link_text(<span class="string">u"网页爬虫"</span>).click()</span><br><span class="line"></span><br><span class="line">html = driver.page_source           <span class="comment"># get html</span></span><br><span class="line">driver.get_screenshot_as_file(<span class="string">"./img/sreenshot2.png"</span>)</span><br><span class="line">driver.close()</span><br><span class="line">print(html[:<span class="number">200</span>])</span><br></pre></td></tr></table></figure></p>
<h3 id="Scrapy爬虫库-需要拓展"><a href="#Scrapy爬虫库-需要拓展" class="headerlink" title="Scrapy爬虫库 需要拓展"></a>Scrapy爬虫库 需要拓展</h3><p>是个很大的框架<br><a href="https://www.jianshu.com/p/a8aad3bf4dc4" target="_blank" rel="noopener">https://www.jianshu.com/p/a8aad3bf4dc4</a><br><a href="https://blog.csdn.net/u012150179/article/details/32343635" target="_blank" rel="noopener">https://blog.csdn.net/u012150179/article/details/32343635</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MofanSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"mofan"</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'https://morvanzhou.github.io/'</span>,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># unseen = set()</span></span><br><span class="line">    <span class="comment"># seen = set()      # we don't need these two as scrapy will deal with them automatically</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> &#123;     <span class="comment"># return some results</span></span><br><span class="line">            <span class="string">'title'</span>: response.css(<span class="string">'h1::text'</span>).extract_first(default=<span class="string">'Missing'</span>).strip().replace(<span class="string">'"'</span>, <span class="string">""</span>),</span><br><span class="line">            <span class="string">'url'</span>: response.url,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        urls = response.css(<span class="string">'a::attr(href)'</span>).re(<span class="string">r'^/.+?/$'</span>)     <span class="comment"># find all sub urls</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(url, callback=self.parse)     <span class="comment"># it will filter duplication automatically</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># lastly, run this in terminal</span></span><br><span class="line"><span class="comment"># scrapy runspider 5-2-scrapy.py -o res.json</span></span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/10/25/python-爬虫基础/" data-id="cl3iva31q00054oa394lpllpo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/03/25/Tensorflow-学习笔记/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Tensorflow 学习笔记
        
      </div>
    </a>
  
  
    <a href="/2017/10/15/二律背反/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">二律背反</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js"></script>



  </div>
</body>
</html>